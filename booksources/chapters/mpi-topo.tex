% -*- latex -*-
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%
%%%% This text file is part of the source of 
%%%% `Parallel Programming in MPI and OpenMP'
%%%% by Victor Eijkhout, copyright 2012-7
%%%%
%%%% mpi-topo.tex : about communicator topologies
%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In the communicators you have seen so far, processes are linearly ordered.
In some circumstances the problem you are coding has some structure,
and expressing the program
in terms of that structure would be convenient. For this purpose, MPI 
can define a virtual \indexterm{topology}. There are two types:
\begin{itemize}
\item regular, Cartesian, grids; and
\item general graphs.
\end{itemize}

\mpiRoutineRef{MPI_Topo_test}

\Level 0 {Cartesian grid topology}
\label{sec:cartesian}

A \indextermsub{Cartesian}{grid} is a structure, typically in 2~or~3 dimensions,
of points that have two neighbours in each of the dimensions.
Thus, if a Cartesian grid has sizes $K\times M\times N$, its
points have coordinates $(k,m,n)$ with $0\leq k<K$ et cetera.
Most points have six neighbours $(k\pm1,m,n)$, $(k,m\pm1,n)$, $(k,m,n\pm1)$;
the exception are the edge points. A~grid where edge processors
are connected through \indexterm{wraparound connections} is called
a \indextermsub{periodic}{grid}.

The most common use of Cartesian coordinates
is to find the rank of process by referring to it in grid terms.
For instance, one could ask `what are my neighbours offset by $(1,0,0)$, 
$(-1,0,0)$, $(0,1,0)$ et cetera'.

While the Cartesian topology interface is fairly easy to use, as
opposed to the more complicated general graph topology below, it is
not actually sufficient for all Cartesian graph uses. Notably, in
a so-called \indextermsub{star}{stencil}, such as the
\indextermsub{nine-point}{stencil}, there are diagonal connections,
which can not be described in a single step. Instead, it is necessary
to take a separate step along each coordinate dimension. In higher
dimensions this is of course fairly awkward.

Thus, even for Cartesian structures, it may be advisable to use the
general graph topology interface.

\Level 1 {Cartesian routines}

The cartesian topology is specified by giving
\indexmpishow{MPI_Cart_create} the sizes of the processor grid along
each axis, and whether the grid is periodic along that axis.
\begin{verbatim}
int MPI_Cart_create(
  MPI_Comm comm_old, int ndims, int *dims, int *periods, 
  int reorder, MPI_Comm *comm_cart)
\end{verbatim}
Each point in this new communicator has a coordinate and a rank.  They
can be queried with \indexmpishow{MPI_Cart_coord} and
\indexmpishow{MPI_Cart_rank} respectively.
\begin{verbatim}
int MPI_Cart_coords(
  MPI_Comm comm, int rank, int maxdims,
  int *coords);
int MPI_Cart_rank(
  MPI_Comm comm, init *coords, 
  int *rank);
\end{verbatim}
Note that these routines can give the coordinates for any rank,
not just for the current process.
%
\verbatimsnippet{cart}

The \n{reorder} parameter to \n{MPI_Cart_create}
indicates whether processes can have a rank
in the new communicator that is different from in the old one.

Strangely enough you can only shift in one direction, you can not
specify a shift vector.
\begin{verbatim}
int MPI_Cart_shift(MPI_Comm comm, int direction, int displ, int *source, 
                  int *dest)
\end{verbatim}
If you specify a processor outside the grid
the result is \indexmpishow{MPI_PROC_NULL}.

\Level 0 {Distributed graph topology}
\label{sec:mpi-dist-graph}

MPI communicators have a topology type associated. This is tested with
%
\indexmpishow{MPI_Topo_test}
%
and possible values are:
\begin{itemize}
\item \indexmpishow{MPI_UNDEFINED} for communicators where nothing
  topology has explicitly been specified.
\item \indexmpishow{MPI_CART} for Cartesian toppologies;
  section~\ref{sec:cartesian}.
\item \indexmpishow{MPI_GRAPH} for the MPI-1 graph topology;
  section~\ref{sec:mpi-1-graph}.
\item \indexmpishow{MPI_DIST_GRAPH} for the distributed graph
  topology; section~\ref{sec:mpi-dist-graph}.
\end{itemize}

\Level 1 {Creation}

There are two creation routines for process graphs.
\begin{itemize}
\item \indexmpishow{MPI_Dist_graph_create_adjacent} assumes that a
  process knows both who it is sending it, and who will send to
  it. This means that every edge in the communication graph is
  represented twice, so the memory footprint is double of what is
  strictly necessary. However, no communication is needed to build the
  graph.
\item \indexmpishow{MPI_Dist_graph_create} specifies on each process
  only its `sources'. Consequently, some amount of processing
  --~including communication~-- is needed to build the full graph.
\end{itemize}

\mpiRoutineRef{MPI_Dist_graph_create}

\Level 1 {Query}

Statistics query:
%
\indexmpishow{MPI_Dist_graph_neighbors_count}

\Level 0 {Graph topology (deprecated)}
\label{sec:mpi-1-graph}

The original \indextermbus{MPI}{1} had a graph topology interface
which required each process to specify the full process graph. Since
this is not scalable, it should be considered deprecated. Use the
distributed graph topology (section~\ref{sec:mpi-dist-graph}) instead.

\mpiRoutineRef{MPI_Graph_create}

