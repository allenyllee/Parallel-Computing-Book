% -*- latex -*-
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%
%%%% This text file is part of the source of 
%%%% `Parallel Programming in MPI and OpenMP'
%%%% by Victor Eijkhout, copyright 2012-2020
%%%%
%%%% petsc-solver.tex : linear system solvers
%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Probably the most important activity in PETSc is solving a linear
system. This is done through a solver object: an object of the class
\indexpetscdef{KSP}. (This stands for Krylov SPace solver.) The solution routine
\lstinline{KSPSolve} takes a matrix and a right-hand-side and gives a
solution; however, before you can call this some amount of setup is needed.

There two very different ways of solving a
linear system: through a direct method, essentially a variant of
Gaussian elimination; or through an iterative method that makes
successive approximations to the solution. In PETSc there are only
iterative methods. We will show how to achieve direct methods later.
The default linear system solver in PETSc is fully parallel, and will
work on many linear systems, but there are many settings and
customizations to tailor the solver to your specific problem.

\Level 0 {KSP: linear system solvers}
\label{sec:petsc-ksp}

\Level 1 {Math background}
\label{sec:petsc-math}

Many scientific applications boil down to the solution of a system of
linear equations at some point:
\[ ?_x\colon Ax=b \]
The elementary textbook way of solving this is through an
\indexterm{LU factorization},
also known as \indexterm{Gaussian elimination}:
\[ LU\leftarrow A,\qquad Lz=b,\qquad Ux=z. \]
While PETSc has support for this, its basic design is geared towards
so-called iterative solution methods.
Instead of directly computing
the solution to the system, they compute a sequence of approximations
that, with luck, converges to the true solution:

\begin{quote}
  \begin{tabbing}
    while \=not converged\\
    \> $x_{i+1}\leftarrow f(x_i)$
  \end{tabbing}
\end{quote}

The interesting thing about iterative methods is that the iterative step
only involved the \indexterm{matrix-vector product}:

\begin{quote}
  \begin{tabbing}
    while \=not converged\\
    \> $r_i = Ax_i-b$\\
    \> $x_{i+1}\leftarrow f(r_i)$
  \end{tabbing}
\end{quote}

This \indexterm{residual} is also crucial in determining whether to stop the iteration:
since we (clearly) can not measure the distance to the true solution, we use
the size of the residual as a proxy measurement.

The remaining point to know is that iterative methods feature a \indexterm{preconditioner}.
Mathematically this is equivalent to transforming the linear system to
\[ M\inv Ax=M\inv b \]
so conceivably we could iterate on the preconditioned matrix and right-hand side.
However, in practice we apply the preconditioner in each iteration:

\begin{quote}
  \begin{tabbing}
    while \=not converged\\
    \> $r_i = Ax_i-b$\\
    \> $z_i = M\inv r_i$\\
    \> $x_{i+1}\leftarrow f(z_i)$
  \end{tabbing}
\end{quote}

In this schematic presentation we have left the nature of the $f()$ update
function unspecified. Here, many possibilities exist; the primary
choice here if of the iterative method type, such as `conjugate gradients',
`generalized minimum residual', or `bi-conjugate gradients stabilized'.

\Level 1 {Solver objects}

First we create a KSP object, which contains the coefficient matrix,
and various parameters such as the desired accuracy,
as well as method specific parameters:
%
\indexpetscref{KSPCreate}.

After this, the basic scenario is:
\begin{lstlisting}
Vec rhs,sol;
KSP solver;
KSPSetOperators(solver,A,A);
KSPSolve(solver,rhs,sol);
\end{lstlisting}
using various default settings. The vectors and the matrix have to be
conformly partitioned.

Since neither
solution nor solution speed is guaranteed, an iterative solver is
subject to some tolerances:
\begin{itemize}
\item a relative tolerance for when the residual has been reduced
  enough;
\item an absolute tolerance for when the residual is objectively
  small;
\item a divergence tolerance that stops the iteration if the residual
  grows by too much; and
\item a bound on the number of iterations, regardless any progress the
  process may still be making.
\end{itemize}

These tolerances are set with \indexpetscref{KSPSetTolerances}.

\Level 1 {Why did my solver stop? Did it work?}
\label{sec:ksp-reason}

On return of the \indexmpishow{KSPSolve} routine there is no guarantee
that the system was successfully solved.
Therefore, you need to invoke
\indexpetscref{KSPGetConvergedReason}
to get a \indexpetscshow{KSPConvergedReason} parameter that indicates
what state the solver stopped in:
\begin{itemize}
\item The iteration can have successfully converged; this corresponds
  to \lstinline{reason}$>0$;
\item the iteration can have diverged, or otherwise failed: \lstinline{reason}$<0$;
\item or the iteration may have stopped at the maximum number of
  iterations while still making progress; \lstinline{reason}$=0$.
\end{itemize}
For more detail, \indexpetscshow{KSPReasonView} can print out the
reason in readable form; for instance
\begin{lstlisting}
KSPReasonView(solver,PETSC_VIEWER_STDOUT_WORLD);
\end{lstlisting}
(This can also be activated with the \n{-ksp_converged_reason}
commandline option.)

In case of successful convergence, you can use \indexpetscshow{KSPGetIterationNumber}
to report how many
iterations were taken.

The following snippet analyzes the status of a \lstinline{KSP} object
that has stopped iterating:
%
\cverbatimsnippet[code/petsc/c]{petscreasonreport}

\Level 1 {Choice of iterator}

There are many iterative methods, and it takes a few function calls to fully specify them:

\petscRoutineRef{KSPSetType}

Here are some values:
\begin{itemize}
\item \lstinline{KSPCG}: only for symmetric positive definite systems.
\item \lstinline{KSPGMRES}: a minimization method that works fairly
  generally; has high memory demands.
\item \lstinline{KSPBCGS}: a quasi-minimization method; uses less memory than GMRES.
\end{itemize}

\Level 1 {Preconditioners}

Another part of an iterative solver is the \indextermdef{preconditioner}.
The mathematical background of this
is given in section~\ref{sec:petsc-math}.
The preconditioner acts to make the coefficient matrix better conditioned,
which will improve the convergence speed; it can even be that without
a suitable preconditioner a solver will not converge at all.

\Level 2 {Background}

The mathematical requirement that the preconditioner~$M$
satisfy $M\approx A$ can take two forms:
\begin{enumerate}
\item We form an explicit approximation to~$A\inv$; this is known as a
  \indexterm{sparse approximate inverse}.
\item We form an operator~$M$ (often given in factored or other
  implicit) form, such that $M\approx A$, and solving a system $Mx=y$
  for~$x$ can be done relatively quickly.
\end{enumerate}

In deciding on a preconditioner, we now have to balance the following factors.
\begin{enumerate}
\item What is the cost of constructing the preconditioner? This should
  not be more than the gain in solution time of the iterative method.
\item What is the cost per iteration of applying the preconditioner?
  There is clearly no point in using a preconditioner that decreases
  the number of iterations by a certain amount, but increases the cost
  per iteration much more.
\item Many preconditioners have parameter settings that make these
  considerations even more complicated: low parameter values may give
  a preconditioner that is cheaply to apply but does not improve
  convergence much, while large parameter values make the application
  more costly but decrease the number of iterations.
\end{enumerate}

\Level 2 {Usage}

Unlike most of the other PETSc object types, a~\lstinline{PC} object
is typically not explicitly created. Instead, it is created as part of
the \lstinline{KSP} object, and can be retrieved from it.

\begin{lstlisting}
PC prec;
KSPGetPC(solver,&prec);
PCSetType(prec,PCILU);
\end{lstlisting}

Beyond setting the type of the preconditioner, there are various
type-specific routines for setting various parameters. Some of these
can get quite tedious, and it is more convenient to set them through
commandline options.

\Level 2 {Types}

Here are some of the available preconditioner types.

As part of the \indexterm{hypre} package (which needs to be installed
during configuration time), \lstinline{PCHYPRE},

\Level 3 {Sparse approximate inverses}

The inverse of a sparse matrix (at least, those from \acp{PDE}) is typically dense.
Therefore, we aim to construct a \indexterm{sparse approximate inverse}.

PETSc offers two such preconditioners, both of which require an external package.
\begin{itemize}
\item \lstinline{PCSPAI}. This is a preconditioner that can only be
  used in single-processor runs, or as local solver in a block
  preconditioner; section~\ref{sec:petsc-bjac}.
\item As part of the \lstinline{PCHYPRE} package, the parallel variant
  \indexterm{parasails} is available.
\end{itemize}

\Level 3 {Incomplete factorizations}

The $LU$ factorization of a matrix stemming from \acp{PDE} problems
has several practical problems:
\begin{itemize}
\item It takes (considerably) more storage space than the coefficient matrix, and
\item it correspondingly takes more time to apply.
\end{itemize}
For instance, for a three-dimensional \ac{PDE} in $N$~variables, the coefficient matrix
can take storage space~$7N$, while the $LU$ factorization takes~$O(N^{5/3})$.

For this reason, often incompletely $LU$ factorizations are popular.
\begin{itemize}
\item PETSc has natively a \lstinline{PCILU} type, but this can only be used sequentially.
\item As part of \emph{hypre}, \emph{pilut}\index{hypre!pilut} is a parallel ILU.
\end{itemize}

\Level 3 {Block methods}
\label{sec:petsc-bjac}

\begin{itemize}
\item \lstinline{PCBJACOBI}: block Jacobi
\item \lstinline{PCASM}: additive Schwarz method
\end{itemize}

\Level 0 {Direct solvers}

PETSc has some support for direct solvers, that is, variants of LU
decomposition. In a sequential context, the \lstinline{PCLU}
preconditioner can be use for this: a direct solver is equivalent to
an iterative method that stops after one preconditioner
application. This can be forced by specifying a KSP type of
\lstinline{KSPPREONLY}.

Distributed direct solvers are more complicated. PETSc does not have
this implemented in its basic code, but it becomes available by
configuring PETSc with the
\indexterm{scalapack} library.

You need to specify which package provides the LU factorization:

\begin{lstlisting}
PCFactorSetMatSolverType(pc, <solvertype> )
\end{lstlisting}

where solvertype can be mumps, superlu, umfpack, or a number of
others. Note that availability of these packages depends on how PETSc
was installed on your system.

\Level 0 {Control through command line options}

From the above you may get the impression that there are lots of calls
to be made to set up a PETSc linear system and solver. And what if you
want to experiment with different solvers, does that mean that you
have to edit a whole bunch of code? Fortunately, there is an easier
way to do things. If you call the routine
%
\indexpetscref{KSPSetFromOptions}
with the \lstinline{solver} as argument,
%
PETSc will look at your command line options and take those into
account in defining the solver. Thus, you can either omit setting
options in your source code, or use this as a way of quickly
experimenting with different possibilities. Example:

\begin{verbatim}
myprogram -ksp_type gmres -ksp_type_gmres_restart 20 -ksp_max_it 200 \
  -pc_type ilu -pc_type_ilu_levels 3
\end{verbatim}

