% -*- latex -*-
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%
%%%% This text file is part of the source of 
%%%% `Parallel Computing'
%%%% by Victor Eijkhout, copyright 2012-6
%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\Level 0 {Working with global information}
\index{collectives|(}

If all processes have individual data, for instance the result
of a local computation, you may want to bring that information
together, for instance to find the maximal computed value
or the sum of all values. Conversely, sometimes one processor has
information that needs to be shared with all.
For this sort of operation, MPI
has \indexterm{collectives}.

There are various cases,
\begin{figure}[ht]
  \includegraphics[scale=.8]{collective_comm}  
  \caption{The four most common collectives}
  \label{fig:collectives}
\end{figure}
the most common ones are illustrated in figure~\ref{fig:collectives}.

Above, you saw how each process can perform its own computation
with its own result. You may want to summarize these results
on one process, known as the \indexterm{root process},
for instance to print them out.
If you perform an operation on the data from the processors,
for instance to compute the maximum value, this is known
as a \indexterm{reduction} (section~\ref{sec:bcast}).
On the other hand, if you need to collect and preserve
all computation results, the operation is known
as a \indexterm{gather} (section~\ref{sec:gatherscatter}).

Conversely, one process can have data that needs to be
spread to all others, for instance because it reads it from file.
If the same item needs to be sent to all processes, this
is known as \indexterm{broadcast}.
If the root process sends individual data to each process,
it is called a \indexterm{scatter}.

\begin{exercise}
  \label{ex:collective-cases}
  How would you realize the following scenarios with MPI collectives?
  \begin{itemize}
  \item Let each process compute a random number. You want to print the
    maximum of these numbers to your screen.
  \item Each process computes a random number again. Now you want to
    scale these numbers by their maximum. 
  \item Let each process compute a random number. You want to print on what processor the
    maximum value is computed. 
  \end{itemize}
\end{exercise}

Collectives are operations that involve all processes in a
communicator. (See section~\ref{sec:collective} for an informal listing.)
A~collective is a
single call, and it blocks on all processors.
That does not mean that
all processors exit the call at the same time: because of
implementational details and network
latency they need not be synchronized in their execution.
However, semantically we can say that
a~process can not finish
a collective until every other process has at least started the collective.

In addition to these collective operations, there are operations that
are said to be `collective on their communicator', but which do not
involve data movement. Collective then means that all processors must
call this routine; not to do so is an error that will 
manifest itself in `hanging' code. One such example is
\n{MPI_Win_fence}.

There are more collectives or variants on the above.
\begin{itemize}
\item If you want to gather or scatter information, but the contribution
  of each processor is of a different size, there are `variable' collectives;
  they have a~\n{v} in the name (section~\ref{sec:v-collective}).
\item Sometimes you want a reduction with partial results, where each processor
  computes the sum (or other operation) on the values of lower-numbered processors.
  For this, you use a \indexterm{scan} collective (section~\ref{sec:scan}).
\item If every processor needs to broadcast to every other, you use an
  \indexterm{all-to-all} operation (section~\ref{sec:allreduce}).
\item A barrier is an operation that makes all processes wait until every
  process has reached the barrier (section~\ref{sec:barrier}).
\end{itemize}

Finally, there are some advanced topics in collectives.
\begin{itemize}
\item Non-blocking collectives; section~\ref{sec:mpi3collect}.
\item User-defined reduction operators; section~\ref{sec:mpi:op-create}.
\end{itemize}

%% \Level 0 {Rooted collectives: broadcast, reduce}
\input chapters/mpi-bcastreduce

%% \Level 0 {Rooted collectives: gather and scatter}
\input chapters/mpi-gatherscatter

%% \Level 0 {Variable-size-input collectives}
\input chapters/mpi-vcollective

%% \Level 0 {Scan operations}
\input chapters/mpi-scan

%% more stuff
\input chapters/mpi-morecollective

\Level 0 {`All'-type collectives}
\commandref{allreduce}

In many applications the result of a collective is needed on all processes.
For instance, if $x,y$ are distributed vector objects, and you want to compute
\[ y- (x^ty)x \]
you need the inner product value on all processors. You could do this
by writing a reduction followed by a broadcast, but more efficient
algorithms exist.  Surprisingly, an `all-gather' operation takes as
long as a rooted gather (see~\HPSCref{sec:collective} for details).

Thus, MPI has the following operations:
\begin{itemize}
\item \indexmpishow{MPI_Allreduce} is equivalent to a \indexmpishow{MPI_Reduce} followed by a broadcast.
\item \indexmpishow{MPI_Allgather} is equivalent to a \indexmpishow{MPI_Gather} followed by a broadcast.
\item \indexmpishow{MPI_Allgatherv} is equivalent to an \indexmpishow{MPI_Gatherv} followed by a broadcast.
\item \indexmpishow{MPI_Alltoall}, \indexmpishow{MPI_Alltoallv}.
\end{itemize}

\index{collectives|)}
