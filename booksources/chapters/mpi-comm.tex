% -*- latex -*-
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%
%%%% This text file is part of the source of 
%%%% `Parallel Programming in MPI and OpenMP'
%%%% by Victor Eijkhout, copyright 2012-2020
%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\Level 0 {Communicator basics}
\label{sec:comm-basic}

\input chapters/mpi-commbasic

\Level 0 {Subcommunications}
\label{sec:communicators}
\index{communicator|(}

In many scenarios you divide a large job over all the available processors.
However, your job has two or more parts that can be considered as
jobs by themselves. In that case it makes sense to divide your processors
into subgroups accordingly.

Suppose for instance that you are running a simulation where inputs are generated,
a~computation is performed on them, and the results of this computation
are analyzed or rendered graphically. You could then consider dividing your
processors in three groups corresponding to generation, computation, rendering.

As long as you only do sends and receives, this division works fine. However,
if one group of processes needs to perform a collective operation, you don't
want the other groups involved in this. Thus, you really want the three groups
to be really distinct from each other.

In order to make such subsets of processes, MPI has the mechanism of
taking a subset of \indexmpishow{MPI_COMM_WORLD} and turning that subset
into a new communicator.

Now you understand why the MPI collective calls had an argument for the
communicator: a~collective involves all processes of that communicator.
By making a communicator that contains a subset of all available processes,
you can do a collective on that subset.

\Level 1 {Scenario: distributed linear algebra}

For \emph{scalability} reasons, matrices should often be distributed
in a 2D manner, that is, each process receives a subblock that is not
a block of columns or rows. This means that the processors themselves
are, at least logically, organized in a 2D grid. Operations
then involve reductions or broadcasts inside rows or columns. For
this, a row or column of processors needs to be in a subcommunicator.

\Level 1 {Scenario: climate model}

A climate simulation code has several components, for instance corresponding
to land, air, ocean, and ice. You can imagine that each needs a different set
of equations and algorithms to simulate. You can then divide your processes,
where each subset simulates one component of the climate, occasionally communicating
with the other components.

\Level 1 {Scenario: quicksort}

The popular quicksort algorithm works by splitting the data
into two subsets that each can be sorted individually.
If you want to sort in parallel, you could implement this by making two subcommunicators,
and sorting the data on these, creating recursively more subcommunicators.

\Level 1 {Shared memory}

There is an important application of communicator splitting in the
context of one-sided communication, grouping processes by whether they
access the same shared memory area; see section~\ref{mpi-comm-split-type}.

%% \Level 0 {Creating new communicators}

%% There are various ways of making new communicators. We discuss three 
%% mechanisms, from simple to complicated.

\input chapters/mpi-commdup

\input chapters/mpi-commsplit

\input chapters/mpi-commgroup

\input chapters/mpi-intercomm

\Level 0 {Review questions}

For all true/false questions, if you answer that a statement is false,
give a one-line explanation.

\begin{enumerate}

\item True or false: in each communicator, processes are numbered consecutively from zero.
\item If a process is in two communicators, it has the same rank in
both.

\end{enumerate}



\index{communicator|)}

