% -*- latex -*-
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%
%%%% This text file is part of the source of 
%%%% `Parallel Programming in MPI and OpenMP'
%%%% by Victor Eijkhout, copyright 2012-9
%%%%
%%%% mpi-commsplit.tex : about splitting communicators
%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\Level 0 {Splitting a communicator}
\label{sec:comm-split}

Splitting a communicator into multiple disjoint communicators
can be done with \indexmpishow{MPI_Comm_split}.
This uses a `colour':
%
\mpiRoutineRef{MPI_Comm_split}
%
and all processes in the old communicator with the same colour
wind up in a new communicator together. The old communicator still exists,
so processes now have two different contexts in which to communicate.

The ranking of processes in the new communicator is determined by a `key' value.
Most of the time, there is no reason to use a relative ranking that is different from
the global ranking, so the \indexmpishow{MPI_Comm_rank} value of the global communicator
is a good choice.

Here is one example of communicator splitting. Suppose your processors
are in a two-dimensional grid:
\begin{lstlisting}
MPI_Comm_rank( MPI_COMM_WORLD, &mytid );
proc_i = mytid % proc_column_length;
proc_j = mytid / proc_column_length;
\end{lstlisting}
You can now create a communicator per column:
\begin{lstlisting}
MPI_Comm column_comm;
MPI_Comm_split( MPI_COMM_WORLD, proc_j, mytid, &column_comm );
\end{lstlisting}
and do a broadcast in that column:
\begin{lstlisting}
MPI_Bcast( data, /* tag: */ 0, column_comm );
\end{lstlisting}
Because of the SPMD nature of the program, you are now doing in parallel
a broadcast in every processor column. Such operations often appear
in \indexterm{dense linear algebra}.

The \indexmpishow{MPI_Comm_split} routine has a `\n{key}' parameter,
which controls how the processes in the new communicator are
ordered. By supplying the rank from the original communicator you let
them be arranged in the same order.

\begin{figure}[ht]
  \includegraphics[scale=.1]{procgrid-bcast}
  \caption{Row and column broadcasts in subcommunicators}
  \label{fig:procgrid-bcast}
\end{figure}

One application of communicator splitting is setting up a processor
grid, with the possibility of using MPI solely within one row or
column; see figure~\ref{fig:procgrid-bcast}.

\begin{exercise}
  \label{ex:rowcolcomm}
  Organize your processes in a grid, and make subcommunicators for
  the rows and columns. For this compute the row and column number of
  each process.

  In the row and column communicator, compute the rank. For instance,
  on a $2\times3$ processor grid you should find:
\begin{verbatim}
Global ranks:  Ranks in row:  Ranks in colum:
  0  1  2      0  1  2        0  0  0
  3  4  5      0  1  2        1  1  1
\end{verbatim}

  Check that the rank in the row communicator is the column number,
  and the other way around.

  Run your code on different number of processes, for instance a
  number of rows and columns that is a power of~2, or that is a prime number.
\begin{tacc}
    This is one occasion where you could use \n{ibrun -np 9};
    normally you would \emph{never} put a processor count on \n{ibrun}.
\end{tacc}
\end{exercise}

As an example of communicator splitting, consider the recursive
algorithm for \indextermbus{matrix}{transposition}.
Processors are organized in a square grid. The matrix is divided
on $2\times 2$ block form.

\begin{exercise}
  \label{ex:recursivetranspose}
  Implement a recursive algorithm for matrix transposition:
  
  \includegraphics[scale=.1]{recursive-transpose}

  \begin{itemize}
  \item Swap blocks $(1,2)$ and $(2,1)$; then
  \item Divide the processors into four subcommunicators, and
    apply this algorithm recursively on each;
  \item If the communicator has only one process, transpose the matrix in place.
  \end{itemize}
\end{exercise}

There is an important application of communicator splitting in the
context of one-sided communication, grouping processes by whether they
access the same shared memory area; see section~\ref{mpi-comm-split-type}.

