% -*- latex -*-
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%
%%%% This text file is part of the source of 
%%%% `Parallel Programming in MPI and OpenMP'
%%%% by Victor Eijkhout, copyright 2012-6
%%%%
%%%% mpiref.tex : MPI reference
%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

This section gives reference information and illustrative examples
of the use of MPI. While the code snippets given here should be enough,
full programs can be found in the repository for this book
\url{https://bitbucket.org/VictorEijkhout/parallel-computing-book}.

\Level 0 {Communicators}

\Level 1 {Communicator duplication}
\label{sec:ref:comm-dup}

In section~\ref{sec:mpi-semantics} it was explained that MPI messages are 
non-overtaking. This may lead to confusing situations, witness the following snippet:

\verbatimsnippet{wrongcatchmain}

This models a main program that does a simple message exchange, and it
makes two calls to library routines. Unbeknown to the user, the
library also issues send and receive calls, and they turn out to
interfere:

\verbatimsnippet{wrongcatchlib}

Here
\begin{itemize}
\item The main program does a send,
\item the library call \n{function_start} does a send and a receive;
  because the receive can match either send, it is paired with the
  first one;
\item the main program does a receive, which will be paired with the send of the 
  library call;
\item both the main program and the library do a wait call, and in
  both cases all requests are succesfully fulfilled, just not the way
  you intended.
\end{itemize}

The solution is to give the library a separate communicator with
\indexmpishow{MPI_Comm_dup}.
%
\mpiRoutineRef{MPI_Comm_dup}
%
Newly created communicators should be
released again with \indexmpishow{MPI_Comm_free}.

\verbatimsnippet{rightcatchmain}

\Level 1 {Splitting communicators}
\label{sec:ref:comm-split}

The command \indexmpishow{MPI_Comm_split} takes a communicator, and
divides it into a number of disjoint communicators. It does this by
assigning processes to the same subcommunicator if they have the same
user-specified `colour' value.
%
\mpiRoutineRef{MPI_Comm_split}
%
The ranking of processes in the new communicator is determined by a `key' value.
Most of the time, there is no reason to use a relative ranking that is different from
the global ranking, so the \n{MPI_Comm_rank} value of the global communicator
is a good choice.
\verbatimsnippet{commsplitrowcol}

There are some predefined colours, named `types', to use in communicator splitting.
The routine \indexmpishow{MPI_Comm_split_type} looks very much like \n{MPI_Comm_split}:
%
\mpiRoutineRef{MPI_Comm_split_type}
%
but the \n{split_type} parameters has to be from the following (short) list:
\begin{itemize}
\item \indexmpishow{MPI_COMM_TYPE_SHARED}: split the communicator into subcommunicators
  of processes sharing a memory area.
\end{itemize}

\Level 1 {Process topologies}
\label{sec:ref:topology}

\Level 2 {Cartesian grid topology}
\label{sec:ref:cartesian}

The cartesian topology is specified by giving
\indexmpishow{MPI_Cart_create} the sizes of the processor grid along
each axis, and whether the grid is periodic along that axis.
\begin{verbatim}
int MPI_Cart_create(
  MPI_Comm comm_old, int ndims, int *dims, int *periods, 
  int reorder, MPI_Comm *comm_cart)
\end{verbatim}
Each point in this new communicator has a coordinate and a rank.  They
can be queried with \indexmpishow{MPI_Cart_coord} and
\indexmpishow{MPI_Cart_rank} respectively.
\begin{verbatim}
int MPI_Cart_coords(
  MPI_Comm comm, int rank, int maxdims,
  int *coords);
int MPI_Cart_rank(
  MPI_Comm comm, init *coords, 
  int *rank);
\end{verbatim}
Note that these routines can give the coordinates for any rank,
not just for the current process.
%
\verbatimsnippet{cart}

The \n{reorder} parameter to \n{MPI_Cart_create}
indicates whether processes can have a rank
in the new communicator that is different from in the old one.

Strangely enough you can only shift in one direction, you can not
specify a shift vector.
\begin{verbatim}
int MPI_Cart_shift(MPI_Comm comm, int direction, int displ, int *source, 
                  int *dest)
\end{verbatim}
If you specify a processor outside the grid
the result is \indexmpishow{MPI_PROC_NULL}.

\Level 0 {Leftover topics}

\Level 1 {MPI constants}
\index{MPI!constants|(}

MPI has a number of built-in \emph{constants}. These do not all behave
the same.
\begin{itemize}
\item Some are \emph{compile-time}\index{MPI!constants!compile-time}
  constants. Examples are \indexmpishow{MPI_VERSION} and
  \indexmpishow{MPI_MAX_PROCESSOR_NAME}. Thus, they can be used in
  array size declarations, even before \indexmpishow{MPI_Init}.
\item Some \emph{link-time}\index{MPI!constants!link-time}
  constants get their value by MPI initialization, such as
  \indexmpishow{MPI_COMM_WORLD}. Such symbols, which include all
  predefined handles, can be used in initialization expressions.
\item Some link-time symbols can not be used in initialization
  expressions, such as \indexmpishow{MPI_BOTTOM} and \indexmpishow{MPI_STATUS_IGNORE}.
\end{itemize}

For symbols, the binary realization is not defined. For instance,
\indexmpishow{MPI_COMM_WORLD} is of type \indexmpishow{MPI_Comm}, but
the implementation of that type is not specified.

(This section was inspired by
\url{http://blogs.cisco.com/performance/mpi-outside-of-c-and-fortran}.)

\index{MPI!constants|)}

\Level 1 {32-bit size issues}

The \n{size} parameter in MPI routines is defined as an \n{int},
meaning that it is limited to 32-bit quantities.  There are ways
around this, such as sending a number of
\indexmpishow{MPI_Type_contiguous} blocks that add up to more than~$2^{31}$.

\Level 1 {Fortran issues}
\label{sec:ref:mpi-fortran}
\index{MPI!Fortran issues|(}

\Level 2 {Data types}

The equivalent of \indexmpishowsub{MPI_Aint}{in Fortran} is
\begin{verbatim}
integer(kind=MPI_ADDRESS_KIND) :: winsize
\end{verbatim}

\Level 2 {Type issues}

Fortran90 is a strongly typed language, so it is not possible to pass
argument by reference to their address, as C/C++ do with the \n{void*}
type for send and receive buffers. In Fortran this is solved by having
separate routines for each datatype, and providing an \n{Interface} block
in the MPI module. If you manage to request a version that does not exist,
the compiler will display a message like
\begin{verbatim}
There is no matching specific 
subroutine for this generic subroutine call [MPI_Send]
\end{verbatim}

\Level 2 {Byte calculations}
\label{sec:f-sizeof}

Fortran lacks a \n{sizeof} operator to query the sizes of datatypes.
Since sometimes exact byte counts are necessary,
for instance in one-sided communication,
Fortran can use the \indexmpishow{MPI_Sizeof} routine,
for instance for \indexmpishow{MPI_Win_create}:
\begin{verbatim}
call MPI_Sizeof(windowdata,window_element_size,ierr)
window_size = window_element_size*500
call MPI_Win_create( windowdata,window_size,window_element_size,... );
\end{verbatim}

\index{MPI!Fortran issues|)}

\Level 1 {Python issues}
\label{sec:python-stuff}
\index{MPI!Python issues|(}

\Level 2 {Byte calculations}

The \indexmpishow{MPI_Win_create} routine needs a displacement in
bytes. Here is a good way for finding the size of \indexterm{numpy} datatypes:
\begin{verbatim}
numpy.dtype('i').itemsize
\end{verbatim}

\Level 2 {Arrays of objects}

Objects of type \n{MPI.Status} or \n{MPI.Request} often need to be created
in an array, for instance when looping through a number of \n{Isend} calls.
In that case the following idiom may come in handy:
\begin{verbatim}
requests = [ None ] * nprocs
for p in range(nprocs):
  requests[p] = comm.Irecv( ... )
\end{verbatim}

\index{MPI!Python issues|)}

\Level 1 {Cancelling messages}

In section~\ref{sec:mpi-source} we showed a master-worker example where the 
master accepts in arbitrary order the messages from the workers.
Here we will show a slightly
more complicated example, where only the result of the first task to
complete is needed. Thus, we issue an \n{MPI_Recv}
with \indexmpishow{MPI_ANY_SOURCE} as source.  When a result comes, we
broadcast its source to all processes.  All the other workers then use
this information to cancel their message with
an \indexmpishow{MPI_Cancel} operation.

\verbatimsnippet{cancel}

\Level 1 {Constants}

MPI constants such as \n{MPI_COMM_WORLD} or \n{MPI_INT} are not
necessarily statitally defined, such as by a \n{#define} statement:
the best you can say is that they have a value after
\indexmpishow{MPI_Init} or \indexmpishow{MPI_Init_thread}.
That means you can not transfer a compiled MPI file between
platforms, or even between compilers on one platform.
However, a working MPI source on one MPI implementation
will also work on another.

\Level 0 {Context information}
\label{sec:context}

\Level 1 {Processor name}

You can query the \indexterm{hostname} of a processor.
This name need not be unique between different processor ranks.
%
\mpiRoutineRef{MPI_Get_processor_name}
%
Note that you have to pass in the character storage:
the character array must be at least \indexmpishow{MPI_MAX_PROCESSOR_NAME} characters long.
The actual length of the name is returned in the \n{resultlen} parameter.

In C and C++,
\begin{verbatim}
  #define MPI_VERSION 2
  #define MPI_SUBVERSION 2
\end{verbatim}
in Fortran,
\begin{verbatim}
  INTEGER MPI_VERSION, MPI_SUBVERSION
  PARAMETER (MPI_VERSION = 2)
  PARAMETER (MPI_SUBVERSION = 2)
\end{verbatim}
For runtime determination,
\begin{verbatim}
  MPI_GET_VERSION( version, subversion )
  OUT version version number (integer)
  OUT subversion subversion number (integer)

  int MPI_Get_version(int *version, int *subversion)
  MPI_GET_VERSION(VERSION, SUBVERSION, IERROR)
  INTEGER VERSION, SUBVERSION, IERROR
\end{verbatim}

\Level 1 {Attributes}

\mpiRoutineRef{MPI_Attr_get}

Attributes are:
\begin{itemize}
\item \indexmpishow{MPI_UNIVERSE_SIZE}: the total number of processes
  that can be created. This can be more than the size of
  \n{MPI_COMM_WORLD} if the host list is larger than the number of
  initially started processes. See section~\ref{sec:mpi-dynamic}.
\item \indexmpishow{MPI_APPNUM}: if MPI is used in \ac{MPMD} mode, or
  if \indexmpishow{MPI_Comm_spawn_multiple} is used, this attribute
  reports the how-manieth program we are in.
\end{itemize}

\Level 0 {Timing}
\label{sec:ref:mpi-timing}

MPI has a \indexterm{wall clock} timer: \indexmpishow{MPI_Wtime}
%
\mpiRoutineRef{MPI_Wtime}
%
which gives the number of seconds from a certain point in the past.
(Note the absence of the error parameter in the fortran call.)
\verbatimsnippet{pingpong}

The timer has a resolution of \indexmpishow{MPI_Wtick}:
%
\mpiRoutineRef{MPI_Wtick}

Timing in parallel is a tricky issue. For instance, most clusters do
not have a central clock, so you can not relate start and stop times
on one process to those on another. You can test for a global clock as
follows\indexmpi{MPI_WTIME_IS_GLOBAL}:
\begin{verbatim}
int *v,flag;
MPI_Attr_get( comm, MPI_WTIME_IS_GLOBAL, &v, &flag );
if (mytid==0) printf(``Time synchronized? %d->%d\n'',flag,*v);
\end{verbatim}
%\indexmpi{MPI_Wtime} can be either a function or a macro.

%\Level 1 {Profiling}
%\label{sec:ref:profile}

\Level 0 {Multi-threading}
\label{sec:ref:mpi-thread}

Hybrid MPI/threaded codes need to replace \indexmpishow{MPI_Init}
by \n{MPI_Init_thread}:
%
\mpiRoutineRef{MPI_Init_thread}
%
With the \n{required} parameter the user requests a certain level of support,
and MPI reports the actual capabilities in the \n{provided} parameter.

The following constants are defined:
\begin{itemize}
\item \indexmpishow{MPI_THREAD_SINGLE}: each MPI process can only have
  a single thread.
\item \indexmpishow{MPI_THREAD_FUNNELED}: an MPI process can be
  multithreaded, but all MPI calls need to be done from a single
  thread.
\item \indexmpishow{MPI_THREAD_SERIALIZED}: a processes can sustain
  multiple threads that make MPI calls, but these threads can not be
  simultaneous: they need to be for instance in an OpenMP
  \indexterm{critical section}.
\item \indexmpishow{MPI_THREAD_MULTIPLE}: processes can be fully
  generally multi-threaded.
\end{itemize}
These values are monotonically increasing.

After the initialization call, you can query the support level
with \n{MPI_Query_thread}:
%
\mpiRoutineRef{MPI_Query_thread}

In case more than one thread performs communication, the following routine
can determine whether a thread is the main thread:
%
\mpiRoutineRef{MPI_Is_thread_main}
