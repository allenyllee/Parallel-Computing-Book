% -*- latex -*-
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%
%%%% This text file is part of the source of 
%%%% `Parallel Computing'
%%%% by Victor Eijkhout, copyright 2012/3
%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

This section gives reference information and illustrative examples
of the use of MPI. While the code snippets given here should be enough,
full programs can be found in the repository for this book
\url{https://bitbucket.org/VictorEijkhout/parallel-computing-book}.

\Level 0 {Basics}

\Level 1 {OpenMP setup}
\commandreflabel{omp-code}

If you use OMP commands in a program file, be sure to include
the proper header file \indexterm{omp.h}.
\begin{verbatim}
#include "omp.h" // for C
\end{verbatim}

For Fortran:
\begin{verbatim}
use omp_lib
\end{verbatim}
The \indexterm{structured block} that follows a pragma is basically a block 
that gets executed as a whole: you can not jump into or out of it.

\Level 0 {Thread stuff}

\Level 1 {Creating parallel threads}
\commandreflabel{omp-parallel}

\indexpragma{parallel}
\verbatimsnippet{hello-who-omp}

\indexpragma{parallel for}

\Level 0 {Controlling thread data}
\commandreflabel{sec:ompdata}

\indexpragma{shared}

\Level 1 {Private data}
\commandreflabel{omp-private}

Data that is declared private with the \indexpragma{private} directive is
put on a separate \indextermbus{stack}{per thread}. The OpenMP standard
does not dictate the size of these stacks, but beware of \indextermbus{stack}{overflow}.
A~typical default
is a few megabyte; you can control it with the environment variable
\indextermtt{OMP_STACKSIZE}

\indexpragma{firstprivate}
\indexpragma{lastprivate}
\indexpragma{copyin}

\Level 1 {Defaults}

You can add clauses to an \n{omp parallel} pragma
to specify explicitly what variables are private and what variables shared.
Note the cases with default behaviour:
\begin{itemize}
\item Loop variables in an \n{omp for} are private;
\item Local variables in the parallel region are private.
\end{itemize}
You can alter this default behaviour with the \indexclause{default} clause:
\begin{itemize}
\item The \indexclauseoption{default}{none} option is good for debugging, 
  because it forces you to specify for each variable in the parallel region
  whether it's private or shared.
\item The \indexclauseoption{default}{shared} clause means that any private variables
  need to be declared explicitly.
\item The \indexclauseoption{default}{private} clause means that any shared variables
  need to be declared explicitly. This value is not available in~C.
\end{itemize}

\Level 0 {Parallel regions}
\commandreflabel{parallelregion}

\indexpragma{parallel}

To test whether you are in a parallel region, use
\indextermtt{omp_in_parallel}.

The number of threads used can differ between parallel regions. This is known
as \indexterm{dynamic mode}. You can set this with
\indextermtt{omp_set_dynamic} and query with
\indextermtt{omp_get_dynamic}.


\Level 1 {Loop parallelism}
\commandreflabel{omp-for}

A loop prefixed with the \indexpragma{for} or \indexpragma{parallel for} pragma
(they are synonymous) is automatically parallelized.
\begin{itemize}
\item The loop can not contains \n{break}, \n{return}, \n{exit} statements, or
  \n{goto} to a label outside the loop.
\item The \n{continue} statement is allowed.
\item The index update has to be an increment (or decrement) by a fixed amount.
\end{itemize}

\Level 1 {Reductions}
\commandreflabel{reduction}

Arithmetic reductions: $+,*,-,\max,\min$

Logical operator reductions: \n{&,&&,|,||,^}

Fortran: 

\Level 0 {Worksharing}
\commandreflabel{workshare}
\index{worksharing constructs|(textbf}

The OpenMP \emph{worksharing constructs} serve to distribute work
over threads.

\Level 1 {Master and single}

The \indexpragma{master} and \indexpragma{single} constructs
are quite similar. They both indicate that a structured block
is to be executed by only a single thread, and that all other threads
that encounter the block skip it. However, the \n{master} pragma
assigns the execution of the block to a specific thread: the master thread.
Therefore, it is technically not a worksharing construct, and thus
there is no barrier at the end of it.

The \n{single} pragma does have a barrier, which can be removed with
\n{nowait}.

\Level 1 {Sections}

\begin{verbatim}
#pragma omp parallel
{
#pragma omp sections
  {
#pragma omp section
    do_thing_A();
#pragma omp section
    do_thing_B();
#pragma omp section
    do_thing_C();
  }
}
\end{verbatim}
\index{worksharing constructs|)}

\Level 0 {Synchronization}

\begin{itemize}
\item \texttt{barrier}
\item \texttt{critical}
\item \texttt{atomic}
\item \texttt{ordered}
\item \texttt{nowait}
\end{itemize}

\Level 1 {Barriers}

A barrier is a location in the code that needs to be reached
by all threads before any of them can continue. There is 
a directive for declaring an explicit barrier, but 
there are also implicit barriers associated with certain other 
directives.

\Level 2 {Explicit barriers}
\commandreflabel{exbarrier}

You saw an example above where explicit barriers are needed.
You can sometimes replace an explicit barrier by an implicit one:
\begin{verbatim}
#pragma omp parallel
{
  // do something
#pragma omp barrier
  // do something else
}
\end{verbatim}
is equivalent to
\begin{verbatim}
#pragma omp parallel
{
  // do something
}
#pragma omp parallel
  // do something else
}
\end{verbatim}
but the second code has more overhead in creating the team of threads a second time.

\Level 2 {Implicit barriers}

At the end of a parallel region the team of threads is dissolved and
only the master thread continues. Therefore, there is an
\emph{implicit barrier at the end of a parallel region}\index{parallel
  region!barrier at the end of}.

There is some \emph{barrier behaviour}\index{omp!for!barrier
  behaviour} associated with \n{omp for} loops and other
\emph{worksharing constructs}\index{worksharing constructs!implied
  barriers at} (see section~\ref{sec:workshare}).  For instance, there
is an \indexterm{implicit barrier} at the end of the loop. This
barrier behaviour can be cancelled with the \indexclause{nowait}
clause.

You will often see the idiom
\begin{verbatim}
#pragma omp parallel
{
#pragma omp for nowait
  for (i=0; i<N; i++)
    a[i] = // some expression
#pragma omp for
  for (i=0; i<N; i++)
    b[i] = ...... a[i] ......
\end{verbatim}
Here the \n{nowait} clause implies that threads can start on the second loop
while other threads are still working on the first. Since the two loops use the same
schedule here, an iteration that uses \n{a[i]} can indeed rely on it that that 
value has been computed.

\Level 1 {Locks}

Create/destroy:
\begin{verbatim}
omp_init_lock();
omp_destroy_lock();
\end{verbatim}
Set and release:
\begin{verbatim}
omp_set_lock();
omp_unset_lock();
\end{verbatim}
Since the set call is blocking, there is also 
\begin{verbatim}
omp_test_lock();
\end{verbatim}

\Level 0 {Schedules}

The schedule can be declared explicitly, set at runtime
through the \indextermtt{OMP_SCHEDULE} environment variable, or left up to the runtime system
by specifying \n{auto}. Especially in the last two cases  you may want to enquire
what schedule is currently being used with
\indextermtt{omp_get_schedule}. Its mirror call is \indextermtt{omp_set_schedule}.

\Level 0 {Runtime library routine}

\begin{itemize}
\item\indextermtt{omp_set_num_threads}: set the number of threads that OpenMP will use.
\item\indextermtt{omp_get_num_threads}: query the number of threads
  active at the current place in the code; this can be lower than what
  was set with \n{omp_set_num_threads}. For a meaningful answer, this
  should be done in a parallel region.
\item\indextermtt{omp_get_thread_num}
\item\indextermtt{omp_get_max_threads}
\item\indextermtt{omp_in_parallel}: test if you are in a parallel region
\item\indextermtt{omp_set_dynamic(int)}: OpenMP can decide from one
  parallel region to another how many threads to use. Turn on
  with~\n{1} and off with~\n{0}.
\item\indextermtt{omp_get_dynamic}: query whether dynamic mode is on.
\item\indextermtt{omp_get_num_procs}: query the physical number of cores available.
\end{itemize}

\Level 0 {Tasks}
\commandreflabel{omp:task}

\Level 0 {Stuff}

\Level 1 {Environment variables}
\index{OpenMP!environment variables|(}
\begin{itemize}
\item \indextermtt{OMP_NUM_THREADS}
\item \indextermtt{OMP_STACKSIZE} controls the amount of space that is
  allocated as per-thread \indexterm{stack}; the space for private
  variables.
\item \indextermtt{OMP_WAIT_POLICY} determines the behaviour of
  threads that wait, for instance for \indexterm{critical section}:
  \begin{itemize}
  \item \n{ACTIVE} puts the thread in a \indexterm{spin-lock}, where
    it actively checks whether it can continue;
  \item \n{PASSIVE} puts the thread to sleep until the \ac{OS} wakes
    it up.
  \end{itemize}
  The `active' strategy uses CPU while the thread is waiting; on the
  other hand, activating it after the wait is instantaneous. With the
  `passive' strategy, the thread does not use any CPU while waiting,
  but activating it again is expensive. Thus, the passive strategy
  only makes sense if threads will be waiting for a (relatively) long
  time.
\item \indextermtt{OMP_PROC_BIND} with values \n{TRUE} and \n{FALSE}
  can bind threads to a processor. On the one hand, doing so can
  minimize data movement; on the other hand, it may increase load
  imbalance.
\end{itemize}
\index{OpenMP!environment variables|)}

\Level 1 {Timing}

To do \indextermsub{OpenMP}{timing} you can use any system utility;
however there is a dedicated routine \indexcommand{omp_get_wtime}
that express the time since some starting point as a double:
\begin{verbatim}
double omp_get_wtime(void);
\end{verbatim}
To measure a time difference:
\begin{verbatim}
double tstart,tend,duration;
tstart = omp_get_wtime();
// do stuff
tend = omp_get_wtime();
duration = tend-tstart;
\end{verbatim}
The timer resolution is given by:
\begin{verbatim}
double omp_get_wtick(void);
\end{verbatim}

\Level 1 {Affinity}

For performance it can be a good idea to bind threads to specific
processors or cores.  OpenMP (as of \emph{version 3.1}) has a
mechanism for \indextermbus{thread}{affinity}\index{OpenMP!version 3.1!thread
  affinity}:\indextermtt{OMP_PROC_BIND}
\begin{verbatim}
export OMP_PROC_BIND=true  
\end{verbatim}
Apart from this, compilers can have proprietary mechanism; 
e.g., for the intel compiler the variable is
\begin{verbatim}
export KMP_AFFINITY=compact,0
\end{verbatim}
for the sun compiler:
\begin{verbatim}
export SUNW_MP_PROCBIND=TRUE
\end{verbatim}
for gcc (pre-openmp 3.1)
\begin{verbatim}
export GOMP_CPU_AFFINITY=0-63
\end{verbatim}

\Level 1 {Relaxed memory model}
\commandreflabel{omp:flush}

\indexpragma{flush}

\begin{itemize}
\item There is an implicit flush of all variables at the start and end 
  of a \emph{parallel region}\index{parallel region!flush at}.
\item There is a flush at each barrier, whether explicit or implicit,
  such as at the end of a \emph{workshare construct}\index{workshare
    construct!flush at}.
\item At entry and exit of a \emph{critical section}\index{critical
  section!flush at}
\item When a \emph{lock}\index{lock!flush at} is set or unset.
\end{itemize}

