% -*- latex -*-
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%
%%%% This text file is part of the source of 
%%%% `Parallel Programming in MPI and OpenMP'
%%%% by Victor Eijkhout, copyright 2012-9
%%%%
%%%% petsc-design.tex : a tutorial section
%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\Level 0 {What is PETSc and why?}

PETSc is a library with a great many uses, but for now let's say that
it's primarily a library for dealing with the sort of linear algebra
that comes from discretized \acp{PDE}. On a single processor, the
basics of such computations 
can be coded out by a grad student during a semester
course in numerical analysis, but on large scale issues get much more
complicated and a library becomes indispensible.

PETSc's prime justification is then that it helps you realize
scientific computations at large scales, meaning large problem sizes
on large numbers of processors.

There are two points to emphasize here:
\begin{itemize}
\item Linear algebra with dense matrices is relatively simple to
  formulate. For sparse matrices the amount of logistics in dealing
  with nonzero patterns increases greatly. PETSc does most of that for
  you.
\item Linear algebra on a single processor, even a multicore one, is
  managable; distributed memory parallelism is much harder, and
  distributed memory sparse linear algebra operations are doubly
  so. Using PETSc will save you many, many, Many! hours of coding over
  developing everything yourself from scratch.
\end{itemize}

\Level 1 {What is in PETSc?}

The routines in PETSc (of which there are hundreds) can roughly be
divided in these classes:
\begin{itemize}
\item Basic linear algebra tools: dense and sparse matrices, both
  sequential and parallel, their construction and simple operations.
\item Solvers for linear systems, and to a lesser extent nonlinear
  systems; also time-stepping methods.
\item Profiling and tracing: after a successful run, timing for
  various routines can be given. In case of failure, there are
  traceback and memory tracing facilities.
\end{itemize}

\Level 1 {Design philosophy}

PETSc has an object-oriented design, even though it is written
in~C. There are classes of objects, such \lstinline{Mat} for
matrices and \lstinline{Vec} for Vectors, but there is also the
\lstinline{KSP} (for "Krylov SPace solver") class of linear system solvers, and
\lstinline{PetscViewer} for outputting matrices and vectors to screen or file.

Part of the object-oriented design is the polymorphism of objects:
after you have created a \lstinline{Mat} matrix as sparse or dense, all methods
such as MatMult (for the matrix-vector product) take the same
arguments: the matrix, and an input and output vector.

This design where the programmer manipulates a `handle' also means
that the internal of the object, the actual storage of the elements,
is hidden from the programmer. This hiding goes so far that even
filling in elements is not done directly but through function calls:
\begin{lstlisting}
VecSetValue(i,j,v,mode)
MatSetValue(i,j,v,mode)
MatSetValues(ni,is,nj,js,v,mode)
\end{lstlisting}

\Level 1 {Language support}

\Level 2 {C/C++}

PETSc is implemented in C, so there is a natural interface
to~C. There is no separate C++ interface.

\Level 2 {Fortran}

A~\emph{Fortran90}\index{Fortran90!PETSc interface}
interface exists. The \emph{Fortran77}\index{Fortran77!PETSc interface}
interface is only of
interest for historical reasons.

To use Fortran, include both a module and a cpp header file:
\begin{verbatim}
#include "petsc/finclude/petscXXX.h"
use petscXXX
\end{verbatim}
(here \n{XXX} stands for one of the PETSc types, but including
\lstinline{petsc.h} and using \lstinline{use petsc}
gives inclusion of the whole library.)

Variables can be declared with their type (\lstinline{Vec},
\lstinline{Mat}, \lstinline{KSP} et cetera), but internally they are
Fortran \lstinline{Type} objects so they can be declared as such.

Example:
\begin{lstlisting}
#include "petsc/finclude/petscvec.h"
use petscvec
Vec b
type(tVec) x
\end{lstlisting}

\Level 2 {Python}

A \emph{python}\index{Python!PETSc interface} interface was written by
Lisandro Dalcin, and requires separate installation, based on already
defined \indextermtt{PETSC_DIR} and \indextermtt{PETSC_ARCH}
variables.  This can be downloaded at
\url{https://bitbucket.org/petsc/petsc4py/src/master/}, with
documentation at
\url{https://www.mcs.anl.gov/petsc/petsc4py-current/docs/}.

\Level 1 {Documentation}

PETSc comes with a manual in pdf form and web pages with the
documentation for every routine. The starting point is the web page
\url{https://www.mcs.anl.gov/petsc/documentation/index.html}.

There is also a mailing list with excellent support for questions and
bug reports.
\begin{taccnote}
  For questions specific to using PETSc on TACC resources, submit
  tickets to the \emph{TACC}\index{TACC!portal} or
  \indextermbus{XSEDE}{portal}.
\end{taccnote}

\Level 0 {Basics of running a PETSc program}

\Level 1 {Compilation}

A PETSc compilation needs a number of include and library paths,
probably too many to specify interactively. The easiest solution is to
create a makefile:
\begin{verbatim}
include ${PETSC_DIR}/lib/petsc/conf/variables
include ${PETSC_DIR}/lib/petsc/conf/rules
program : program.o
        ${CLINKER} -o $@ $^ ${PETSC_LIB}
\end{verbatim}
The two include lines provide the compilation rule and the library
variable. If you want to write your own compiler rule, use
\begin{verbatim}
include ${PETSC_DIR}/lib/petsc/conf/variables
%.o : %.c
        ${CC} -c $^ ${PETSC_CC_INCLUDES}
program : program.o
        ${CLINKER} -o $@ $^ ${PETSC_LIB}
\end{verbatim}
(The \indexpetscshow{PETSC_CC_INCLUDES} variable contains all paths for
compilation of C~programs; correspondingly there is
\indexpetscshow{PETSC_CC_INCLUDES} for Fortran source.)

The build process assumes that variables \indexpetscshow{PETSC_DIR} and
\indexpetscshow{PETSC_ARCH} have been set. These depend on your local
installation. Usually there will be one installation with debug
settings and one with production settings. Develop your code with the
former: it will do memory and bound checking. Then recompile and run
your code with the optimized production installation.

\begin{taccnote}
  On TACC clusters, a petsc installation is loaded by commands such as
\begin{verbatim}
module load petsc/3.11
\end{verbatim}
Use \n{module avail petsc} to see what configurations exist. The basic
versions are
\begin{verbatim}
# development
module load petsc/3.11-debug
# production
module load petsc/3.11
\end{verbatim}
Other installations are real versus complex, or 64bit integers instead
of the default 32. The command 
\begin{verbatim}
module spider petsc
\end{verbatim}
tells you all the
available petsc versions. The listed modules have a naming convention
such as \n{petsc/3.11-i64debug} where the 3.11 is the PETSc release (minor
patches are not included in this version; TACC aims to install only
the latest patch, but generally several versions are available), and
\n{i64debug} describes the debug version of the installation with 64bit
integers.
\end{taccnote}

\Level 1 {Running}

PETSc programs use MPI for parallelism, so they are started like any other
MPI program:
\begin{verbatim}
mpirun -np 5 -machinefile mf \
    your_petsc_program option1 option2 option3
\end{verbatim}
\begin{taccnote}
  On TACC clusters, use \indextermtt{ibrun}.
\end{taccnote}

\Level 1 {Startup}
\label{sec:petscinit}

PETSc has an call that initializes both PETSc and MPI, so normally you
would replace \indexmpishow{MPI_Init} by
\indexpetscref{PetscInitialize}.
Unlike with MPI, you do not want to
use a \n{NULL} value for the \n{argc,argv} arguments, since PETSc
makes extensive use of commandline options; see
section~\ref{sec:petsc-options}.

\begin{pythonnote}
  The following works if you don't need commandline options.
\begin{verbatim}
from petsc4py import PETSc
\end{verbatim}
To pass commandline arguments to PETSc, do:
\begin{verbatim}
import sys
from petsc4py import init
init(sys.argv)
from petsc4py import PETSc
\end{verbatim}
\end{pythonnote}

After initialization, you can use \indexmpishow{MPI_COMM_WORLD} or
\indexpetscshow{PETSC_COMM_WORLD}:

\begin{lstlisting}
MPI_Comm comm = PETSC_COMM_WORLD;
MPI_Comm_rank(comm,&mytid);
MPI_Comm_size(comm,&ntids);
\end{lstlisting}

\begin{pythonnote}
\begin{verbatim}
comm = PETSc.COMM_WORLD
nprocs = comm.getSize(self) 
procno = comm.getRank(self)
\end{verbatim}
\end{pythonnote}

\Level 0 {PETSc installation}

PETSc has a large number of installation options. These can roughly be
divided into:
\begin{enumerate}
\item Options to describe the environment in which PETSc is being
  installed, such as the names of the compilers or the location of the
  MPI library;
\item Options to specify the type of PETSc installation: real versus
  complex, 32~versus 64-bit integers, et cetera;
\item Options to specify additional packages to download.
\end{enumerate}

For an existing installation, you can find the options used in the
file
\begin{verbatim}
$PETSC_DIR/$PETSC_ARCH/lib/petsc/conf/configure.log
\end{verbatim}

\Level 1 {Environment options}

Compilers, compiler options, MPI.

While it is possible to specify \indexpetscoption{download_mpich},
this should only be done on machines that you are certain do not
already have an MPI library, such as your personal
laptop. Supercomputer clusters are likely to have an optimized MPI
library, and letting PETSc download its own will lead to degraded
performance.

\Level 1 {Variants}

\begin{itemize}
\item Scalars: the option \indexpetscoption{with-scalar-type} has values
  \n{real}, \n{complex}; \indexpetscoption{with-precision} has values
  \n{single}, \n{double}, \n{__float128}, \n{__fp16}.
\end{itemize}
