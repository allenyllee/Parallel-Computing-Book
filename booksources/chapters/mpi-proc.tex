% -*- latex -*-
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%
%%%% This text file is part of the source of 
%%%% `Parallel Programming in MPI and OpenMP'
%%%% by Victor Eijkhout, copyright 2012-9
%%%%
%%%% mpi-proc.tex : about point-to-point communication
%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In this course we have up to now only considered the \ac{SPMD} model
of running MPI programs.  In some rare cases you may want to run in an
\ac{MPMD} mode, rather than \ac{SPMD}. This can be achieved either on
the \ac{OS} level, using options of the \indexterm{mpiexec} mechanism,
or you can use MPI's built-in process management. Read on if you're
interested in the latter.

\Level 0 {Process spawning}
\label{sec:mpi-dynamic}

The first version of MPI did not contain any process management
routines, even though the earlier \indexterm{PVM} project did have
that functionality. Process management was later added with MPI-2.

Unlike what you might think, newly added processes do not become part
of \lstinline$MPI_COMM_WORLD$; rather, they get their own communicator, and an
\indexterm{intercommunicator} is established between this new group
and the existing one. The first routine is
\indexmpishow{MPI_Comm_spawn}, which tries to fire up multiple copies
of a single named executable. You could imagine using this mechanism
to start the whole of your MPI code, but that is likely to be inefficient.

\mpiRoutineRef{MPI_Comm_spawn}

(If you're feeling sure of yourself, specify \indexmpidef{MPI_ERRCODES_IGNORE}.)

Here is an example of a work manager.
%
\cverbatimsnippet{spawnmanager}
%
\pverbatimsnippet{spawnmanagerp}
%
You could start up a single copy of this program with 
\begin{verbatim}
mpirun -np 1 spawn_manager
\end{verbatim}
but with a hostfile that has more than one host. In that case the
\indexmpidef{MPI_UNIVERSE_SIZE} will tell you to the total number of
hosts available. If this option is not supported, you can determine
yourself how many processes you want to spawn. If you exceed the
hardware resources, your multi-tasking operating system (which is some
variant of Unix for almost everyone) will use
\indexterm{time-slicing}, but you will not gain any performance.

\begin{taccnote}
\indextermbus{Intel}{mpi} requires you to pass an option \n{-usize} to
\n{mpiexec} indicating the size of the comm universe. With the TACC
jobs starter \indextermtt{ibrun} do the following:
\begin{verbatim}
MY_MPIRUN_OPTIONS="-usize 8" ibrun -np 4 spawn_manager
\end{verbatim}
\end{taccnote}
The spawned program looks very much like a regular MPI program, with
its own initialization and finalize calls.

\cverbatimsnippet{spawnworker}
%
\pverbatimsnippet{spawnworkerp}

Spawned processes wind up with a value of \lstinline$MPI_COMM_WORLD$ of their
own, but managers and workers can find each other regardless.
The spawn routine returns the intercommunicator to the parent; the children
can find it through \indexmpishow{MPI_Comm_get_parent}. The number of
spawning processes can be found through
\indexmpishow{MPI_Comm_remote_size} on the parent communicator.

\mpiRoutineRef{MPI_Comm_remote_size}

\Level 1 {MPMD}

Instead of spawning a single executable, you can spawn multiple with
\indexmpishow{MPI_Comm_spawn_multiple}.

\Level 0 {Socket-style communications}

It is possible to establish connections with running MPI programs that
have their own world communicator.
\begin{itemize}
\item The \indexterm{server} process establishes a port with 
  \indexmpishow{MPI_Open_port}, and calls \indexmpishow{MPI_Comm_accept} to accept
  connections to its port.
\item The \indexterm{client} process specifies that port 
  in an \indexmpishow{MPI_Comm_connect} call. This establishes the connection.
\end{itemize}

\Level 1 {Server calls}

The server calls \indexmpishow{MPI_Open_port}, yielding a port name.
Port names are generated by the system and copied into a character
buffer of length at most \indexmpidef{MPI_MAX_PORT_NAME}.

\mpiRoutineRef{MPI_Open_port}

The server then needs to call 
\indexmpishow{MPI_Comm_accept} prior to the client doing a connect call.
This is collective over the calling communicator. It returns an
intercommunicator that allows communication with the client.

\mpiRoutineRef{MPI_Comm_accept}

The port can be closed with 
\indexmpishow{MPI_Close_port}.

\Level 1 {Client calls}

After the server has generated a port name, the client 
needs to connect to it with
\indexmpishow{MPI_Comm_connect}, again specifying the port through a character buffer.

\mpiRoutineRef{MPI_Comm_connect}

If the named port does not exist (or has been closed),
\indexmpishow{MPI_Comm_connect} raises an error of class \indexmpishow{MPI_ERR_PORT}.

The client can sever the connection with
\indexmpishow{MPI_Comm_disconnect}

The connect call is collective over its communicator.

\Level 1 {Published service names}

\mpiRoutineRef{MPI_Publish_name}

\indexmpishow{MPI_Unpublish_name}

Unpublishing a non-existing or already unpublished service gives an
error code of \indexmpidef{MPI_ERR_SERVICE}.

\indexmpishow{MPI_Comm_join}

MPI provides no guarantee of fairness in servicing connection
attempts. That is, connection attempts are not necessarily satisfied
in the order in which they were initiated, and competition from other
connection attempts may prevent a particular connection attempt from
being satisfied.

