% -*- latex -*-
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%
%%%% This text file is part of the source of 
%%%% `Parallel Programming in MPI and OpenMP'
%%%% by Victor Eijkhout, copyright 2012-9
%%%%
%%%% mpi-bcastreduce.tex : about broadcast & reduce collectives
%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\Level 0 {Reduction}

\Level 1 {Reduce to all}
\label{sec:allreduce}

Above we saw a couple of scenarios where a quantity is reduced, with
all proceses getting the result. The MPI call for this is:

\mpiRoutineRef{MPI_Allreduce}

Example: we give each process a random number, and sum these numbers together.
The result should be approximately $1/2$ times the number of processes.

\cverbatimsnippet[examples/mpi/c/allreducec.c]{allreducec}

For Python we illustrate both the native and the numpy variant. In the
numpy variant we create an array for the receive buffer, even though
only one element is used.

\cverbatimsnippet[examples/mpi/c/allreducep.c]{allreducep}

\begin{exercise}
  \label{ex:randommaxscale}
  Let each process compute a random number,
  and compute the sum of these numbers using the \indexmpishow{MPI_Allreduce}
  routine.

  (The operator is \indexmpishow{MPI_SUM} for C/Fortran, or \indexmpishow{MPI.SUM} for
  Python.)
  
  Each process then scales its value
  by this sum. Compute the sum of the scaled numbers and check that it is~1.
\end{exercise}

\Level 1 {Reduction of distributed data}
\label{sec:dist-reduc}

One of the more common applications of the reduction operation
is the \indexterm{inner product} computation. Typically, you have two vectors $x,y$
that have the same distribution, that is,
where all processes store equal parts of $x$ and~$y$.
The computation is then
\begin{lstlisting}
local_inprod = 0;
for (i=0; i<localsize; i++)
  local_inprod += x[i]*y[i];
MPI_Allreduce( &local_inprod, &global_inprod, 1,MPI_DOUBLE ... ) 
\end{lstlisting}

\Level 1 {Reduce in place}
\label{sec:allreduce-inplace}

By default MPI will not overwrite the original data with the reduction
result, but you can tell it to do so
using the \indexmpishow{MPI_IN_PLACE} specifier:
%
\cverbatimsnippet[examples/mpi/c/allreduceinplace.c]{allreduceinplace}
%
This has the advantage of saving half the memory.

\Level 1 {Reduction operations}
\label{sec:mpi:op-create}

Several \indexmpishow{MPI_Op} values are pre-defined. For the list,
see section~\ref{sec:operator-list}.

For use in reductions and scans it is possible to define your own operator.

\begin{lstlisting}
MPI_Op_create( MPI_User_function *func, int commute, MPI_Op *op);
\end{lstlisting}

For more details, see section~\ref{sec:mpi-op-create}.

\Level 0 {Rooted collectives: broadcast, reduce}
\label{sec:rooted}

In some scenarios there is a certain process that has a privileged status.
\begin{itemize}
\item
  One process can generate or read in the initial data for a program
  run. This then needs to be communicated to all other processes.
\item
  At the end of a program run, often
  one process needs to output some summary information.
\end{itemize}
This process is called the \indexterm{root} process, and we will now
consider routines that have a root.

\Level 1 {Reduce to a root}
\label{sec:reduce-root}

In the broadcast operation a single data item was communicated to all
processes. Reduction operations go the other way: each process has a
data item, and these are all brought together into a single item.

Here are the essential elements of a reduction operation:
\begin{lstlisting}
MPI_Reduce( senddata, recvdata..., operator,
    root, comm ); 
\end{lstlisting}
\begin{itemize}
\item There is the original data, and the data resulting from the
  reduction. It is a design decision of MPI that it will not by
  default overwrite the original data. The send data and receive data
  are of the same size and type: if every processor has one real
  number, the reduced result is again one real number.
\item There is a reduction operator. Popular choices are
  \indexmpishow{MPI_SUM}, \indexmpishow{MPI_PROD} and
  \indexmpishow{MPI_MAX}, but complicated operators such as finding
  the location of the maximum value exist. You can also define your
  own operators; section~\ref{sec:mpi:op-create}.
\item There is a root process that receives the result of the
  reduction. Since the non-root processes do not receive the reduced
  data, they can actually leave the receive buffer undefined.
\end{itemize}

\cverbatimsnippet[examples/mpi/c/reduce.c]{reduce}

\mpiRoutineRef{MPI_Reduce}

\begin{exercise}
  \label{ex:randommax}
  Write a program where each process computes a random number, and process~0
  finds and prints the maximum generated value. Let each process print its value,
  just to check the correctness of your program.
\begin{book}
  (See~\ref{ch:random} for a discussion of random number generation.)
\end{book}
\end{exercise}

Collective operations can also take an array argument, instead of just a scalar.
In that case, the operation is applied pointwise to each location in the array.

\begin{exercise}
  \label{ex:randomcoord}
  Create on each process an array of length~2 integers, and put the
  values $1,2$ in it on each process. Do a sum reduction on that
  array. Can you predict what the result should be?  Code it. Was your
  prediction right?
\end{exercise}

\Level 1 {Reduce in place}

Instead of using a send and a receive buffer in the reduction, it is
possible to avoid the send buffer by putting the send data in the
receive buffer. We see this mechanism
in section~\ref{sec:allreduce-inplace} for the allreduce operation.

For the rooted call \indexmpishow{MPI_Reduce}, it is similarly
possible to use the value in the receive buffer on the root.
However, on all other processes, data is placed in the send buffer and
the receive buffer is null or ignored as before.

This example sets the buffer values through some pointer cleverness in
order to have the same reduce call on all processes.
%
\cverbatimsnippet[examples/mpi/c/reduceinplace.c]{reduceinplace}

In Fortran the code is less elegant because you can not do
these address calculations:
%
\fverbatimsnippet{reduceinplace-f}

\Level 1 {Broadcast}
\label{sec:bcast}

The broadcast call has the following structure:
\begin{lstlisting}
MPI_Bcast( data..., root , comm);
\end{lstlisting}
The root is the process that is sending its data.
Typically, it will be the root of a broadcast tree.
The \n{comm} argument is a communicator:
for now you can use \indexmpishow{MPI_COMM_WORLD}.
Unlike with send/receive there is no message tag,
because collectives are blocking, so you can have only one collective active at a
time. 

The data in a broadcast (or any other MPI operation for that matter)
is specified as
\begin{itemize}
\item A buffer. In~C this is the address in memory of the data. This means
  that you broadcast a single scalar as \indexmpishow{MPI_Bcast}\n{( &value, ... )},
  but an array as \indexmpishow{MPI_Bcast}\n{( array, ... )}.
\item The number of items and their datatype. The allowable datatypes
  are such things as \indexmpishow{MPI_INT} and \indexmpishow{MPI_FLOAT} for~C, and
  \indexmpishow{MPI_INTEGER} and \indexmpishow{MPI_REAL} for Fortran, or more complicated types.
  See section~\ref{ch:mpi-data} for details.
\end{itemize}
\begin{pythonnote}
  In python it is both possible to send objects, and to send more
  C-like buffers. The two possibilities correspond (see
  section~\ref{sec:python-bind}) to different routine names; the
  buffers have to be created as \n{numpy} objects.
\end{pythonnote}
%
Example: in general we can not assume that all processes get the
commandline arguments, so we broadcast them from process~0.

\cverbatimsnippet[examples/mpi/c/usage.c]{usage}

\mpiRoutineRef{MPI_Bcast}

\begin{exercise}
  \label{ex:argv-bcast}
  If you give a commandline argument to a program, that argument is available
  as a character string as part of the \n{argv,argc} pair that you typically use
  as the arguments to your main program. You can use the function \n{atoi} to
  convert such a string to integer.

  Write a program where process~0 looks for an integer on the commandline, and
  broadcasts it to the other processes. Initialize the buffer on all processes, and
  let all processes print out the broadcast number,
  just to check that you solved the problem correctly.
\end{exercise}

In python we illustrate the native and numpy variants. In the native
variant the result is given as a function return; in the numpy variant
the send buffer is reused.
%
\pverbatimsnippet{bcastp}

\begin{figure}[p]
  \tiny
  \begin{multicols}{3}
    Initial:\\ \nobreak
    \input jordan1
    Step 1:\\ \nobreak
    \input jordan2
    Step 2:\\ \nobreak
    \input jordan3
    Step 3:\\ \nobreak
    \input jordan4
    Step 4:\\ \nobreak
    \input jordan5
    Step 5:\\ \nobreak
    \input jordan6
    Step 6:\\ \nobreak
    \input jordan7
    Step 7:\\ \nobreak
    \input jordan8
    Step 8:\\ \nobreak
    \input jordan9
    Step 9:\\ \nobreak
    \input jordan10
    Step 10:\\ \nobreak
    \input jordan11
    Step 11:\\ \nobreak
    \input jordan12
    Step 12:\\ \nobreak
    \input jordan13
    Step 13:\\ \nobreak
    \input jordan14
    Step 14:\\ \nobreak
    \input jordan15
    Step 15:\\ \nobreak
    \input jordan16
    Step 16:\\ \nobreak
    \input jordan17
    Step 17:\\ \nobreak
    \input jordan18
    Step 18:\\ \nobreak
    \input jordan19
    Finished:\\ \nobreak
    \input jordan20
  \end{multicols}
  \caption{Gauss-Jordan elimination example}
  \label{fig:gauss-jordan-ex}
\end{figure}

For the following exercise, study figure~\ref{fig:gauss-jordan-ex}.

\begin{exercise}
  \label{ex:gaussjordancoll}
  The \indexterm{Gauss-Jordan algorithm} for solving a linear system
  with a matrix~$A$ (or computing its inverse) runs as follows:
  {\small
  \begin{tabbing}
    for \=pivot $k=1,\ldots,n$\\
    \>let the vector of scalings $\ell^{(k)}_i=A_{ik}/A_{kk}$\\
    \>for \=row $r\not=k$\\
    \>\>for \=column $c=1,\ldots,n$\\
    \>\>\> $A_{rc}\leftarrow A_{rc} - \ell^{(k)}_r A_{rc}$\\
  \end{tabbing}
  }
  where we ignore the update of the righthand side, or the formation
  of the inverse.

  Let a matrix be distributed with each process storing one
  column. Implement the Gauss-Jordan algorithm as a series of
  broadcasts: in iteration $k$ process $k$ computes and broadcasts the
  scaling vector~$\{\ell^{(k)}_i\}_i$. Replicate the right-hand side on
  all processors.
\end{exercise}

\begin{exercise}
  Add partial pivoting to your implementation of Gauss-Jordan elimination.

  Change your implementation to let each processor store multiple columns,
  but still do one broadcast per column. Is there a way to have only one
  broadcast per processor?
\end{exercise}

