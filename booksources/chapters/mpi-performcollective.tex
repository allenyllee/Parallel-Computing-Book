% -*- latex -*-
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%
%%%% This text file is part of the source of 
%%%% `Parallel Programming in MPI and OpenMP'
%%%% by Victor Eijkhout, copyright 2012-9
%%%%
%%%% mpi-performcollective.tex : performance issues
%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\Level 0 {Implementation and performance of collectives}

In this section we will consider how collectives can be implemented in
multiple ways, and the performance implications of such decisions.
You can test the algorithms described here using \indexterm{SimGrid}
(section~\ref{tut:simgrid}).

\Level 1 {Broadcast}

\heading{Naive broadcast}

Write a broadcast operation where the root does an \indexmpishow{MPI_Send} to
each other process.

What is the expected performance of this in terms of $\alpha,\beta$?

Run some tests and confirm.

\heading{Simple ring}

Let the root only send to the next process, and that one send to its
neighbour. This scheme is known as a \indexterm{bucket brigade}; see
also section~\ref{sec:bucketbrigade}.

What is the expected performance of this in terms of $\alpha,\beta$?

Run some tests and confirm.

\begin{figure}[ht]
  \includegraphics[scale=.6]{bucketpipe}
  \caption{A pipelined bucket brigade}
  \label{fig:pipe-bucket}
\end{figure}

\heading{Pipelined ring}

In a ring broadcast, each process needs to receive the whole message
before it can pass it on. We can increase the efficiency by breaking
up the message and sending it in multiple parts.
(See figure~\ref{fig:pipe-bucket}.)
This will be
advantageous for messages that are long enough that the bandwidth cost
dominates the latency.

Assume a send buffer of length more than~1. Divide the send buffer
into a number of chunks. The root sends the chunks successively to the
next process, and each process sends on whatever chunks it receives.

What is the expected performance of this in terms of $\alpha,\beta$?
Why is this better than the simple ring?

Run some tests and confirm.

\heading{Recursive doubling}

Collectives such as broadcast can be
\emph{implemented}
through
\index{doubling!recursive|see{recursive doubling}}%
\indexterm{recursive doubling},
where the root sends to another process, then the root and the other
process send to two more, those four send to four more, et cetera.
However, in an actual physical architecture this scheme can be
realized in multiple ways that have drastically different performance.

First consider the implementation where process~0 is the root, and it
starts by sending to process~1; then they send to 2 and~3; these four
send to~4--7, et cetera. If the architecture is a linear array of
procesors, this will lead to \indexterm{contention}: multiple messages
wanting to go through the same wire. (This is also related to the
concept of \indextermsub{bisecection}{bandwidth}.)

In the following analyses we will assume \indexterm{wormhole routing}:
a~message sets up a path through the network, reserving the necessary
wires, and performing a send in time independent of the distance
through the network. That is, the send time for any message can be
modeled as \[ T(n)=\alpha+\beta n \] regardless source and
destination, as long as the necessary connections are available. 

\begin{exercise}
  \label{ex:bcast-doubling-block}
  Analyze the running time of a recursive doubling broad cast as just
  described, with wormhole routing.

  Implement this broadcast in terms of blocking MPI send and receive calls.
  If you have SimGrid available, run
  tests with a number of parameters.
\end{exercise}

The alternative, that avoids contention, is to let each doubling stage
divide the network into separate halves. That is, process~0 sends
to~$P/2$, after which these two repeat the algorithm in the two halves
of the network, sending to $P/4$ and $3P/4$ respectively.

\begin{exercise}
  Analyze this variant of recursive doubling. Code it and measure
  runtimes on SimGrid.
\end{exercise}

\begin{exercise}
  \label{ex:bcast-doubling-nonblock}
  Revisit exercise \ref{ex:bcast-doubling-block} and replace the
  blocking calls by non-blocking \indexmpishow{MPI_Isend}~/ \indexmpishow{MPI_Irecv} calls.

  Make sure to test that the data is correctly propagated.
\end{exercise}
