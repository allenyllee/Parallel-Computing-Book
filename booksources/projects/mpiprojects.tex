% -*- latex -*-
\Level 0 {Mandelbrot set}

If you've never heard the name \indexterm{Mandelbrot set}, you
probably recognize the picture. Its formal definition is as follows:
\begin{quotation}\noindent
  A point $c$ in the complex plane is part of the Mandelbrot set if 
  the series $x_n$ defined by 
  \[ 
  \begin{cases}
    x_0=0\\ x_{n+1}=x_n^2+c
  \end{cases}
  \] satisfies \[ \forall_n\colon |x_n|\leq 2. \]  
\end{quotation}
It is easy to see that only points $c$ in the bounding circle
$|c|< 2$ qualify, but
apart from that it's hard to say much without a lot more thinking.
Or computing; and that's what we're going to do.

In this set of exercises you are going to take an example program
\n{mandel_main.cxx} and extend it to use a variety of MPI programming
constructs.  This program has been set up as a
\indexterm{master-worker} model: there is one master processor (for a
change this is the last processor, rather than zero) which gives out
work to, and accepts results from, the worker processors. It then
takes the resuls and construct an image file from them.

\Level 1 {Invocation}

The \n{mandel_main} program is called as
\begin{verbatim}
mpirun -np 123 mandel_main steps 456 iters 789
\end{verbatim}
where the \n{steps} parameter indicates how many steps in $x,y$
direction there are in the image, and \n{iters} gives the maximum
number of iterations in the \n{belong} test.

If you forget the parameter, you can call the program with
\begin{verbatim}
mandel_serial -h
\end{verbatim}
and it will print out the usage information.

\Level 1 {Tools}

The driver part of the Mandelbrot program is simple. There is a 
circle object that can generate coordinates
\verbatimsnippet{circledef}
and a global routine that tests whether a coordinate is in the set,
at least up to an iteration bound. It returns zero if the 
series from the given starting point has not diverged,
or the iteration number in which it diverged if it did so.
\verbatimsnippet{belongs}
In the former case,  the point could be in the Mandelbrot set, 
and we colour it black, in the latter case we give it a colour
depending on the iteration number.
\verbatimsnippet{pickcolour}
We use a fairly simple code for the worker processes: they 
execute a loop in which they wait 
for input, process it, return the result.
\verbatimsnippet{waitforwork}

A very simple solution using blocking sends on the master is given:
\verbatimsnippet{serialaddtask}

\begin{exercise}
  Explain why this solution is very inefficient. Make a trace of its
  execution that bears this out.
\end{exercise}

\Level 1 {Bulk task scheduling}
\label{proj:mandel-bulk}

The previous section showed a very inefficient solution, but that was mostly
intended to set up the code base. If all tasks take about the same amount of time,
you can give each process a task, and then wait on them all to finish. A~first way
to do this is with non-blocking sends.

\begin{exercise}
  Code a  solution where you give a task to all worker processes
  using non-blocking sends and receives, and then wait for these tasks
  with \n{MPI_Waitall}
  to finish before you give a new round of data to all workers.
  Make a trace of the execution of this and report on the total time.

  You can do this by writing a new class that inherits from \n{queue},
  and that provides its own \n{addtask} method:
\verbatimsnippet{bulkq}
  You will also have to override the \n{complete} method: when the circle 
  object indicates that all coordinates have been generated, not all
  workers will be busy, so you need to supply the proper \n{MPI_Waitall}
  call.
\end{exercise}

\Level 1 {Collective task scheduling}
\label{proj:mandel-collective}

Another implementation of the bulk scheduling of the previous section
would be through using collectives.

\begin{exercise}
  Code a solution which uses scatter to distribute data to the worker
  tasks, and gather to collect the results. Is this solution more or
  less efficient than the previous?
\end{exercise}

\Level 1 {Asynchronous task scheduling}

At the start of section~\ref{proj:mandel-bulk} we said that bulk scheduling
mostly makes sense if all tasks take similar time to complete.
In the Mandelbrot case this is clearly not the case.

\begin{exercise}
  Code a fully dynamic solution that uses \n{MPI_Probe} or \n{MPI_Waitany}.
  Make an execution trace and report on the total running time.
\end{exercise}

\Level 1 {One-sided solution}

Let us reason about whether it is possible (or advisable) to code a
one-sided solution to computing the Mandelbrot set.  
%
With active
target synchronization you could have an exposure window on the host
to which the worker tasks would write. To prevent conflicts you would allocate an 
array and have each worker write to a separate location in it.
%
The problem here is that the workers may not be sufficiently synchronized because
of the differing time for computation.

Consider then passive target synchronization. Now the worker tasks could
write to the window on the master whenever they have something to
report; by locking the window they prevent other tasks from interfering.
%
After a worker writes a result, it can get new data from an array
of all coordinates on the master. 

It is hard to get results into the image as they become available. For
this, the master would continuously have to scan the results
array. Therefore, constructing the image is easiest done when all
tasks are concluded.

\Level 0 {Data parallel grids}

\Level 1 {A realistic programming example}

In this section we will gradually build a semi-realistic example
program. To get you started some pieces have already been written:
as a starting point look at \n{code/mpi/c/grid.cxx}.

\Level 2 {Description of the problem}

With this example you will investigate several strategies for
implementing a simple iterative method. Let's say you have a
two-dimensional grid of datapoints $G=\{g_{ij}\colon 0\leq
i<n_i,\,0\leq j<n_j\}$ and you want to compute~$G'$ where
\begin{equation}
g'_{ij} = 1/4 \cdot (g_{i+1,j}+g_{i-1,j}+g_{i,j+1}+g_{i,j-1}).
\label{eq:grid-update}
\end{equation}

This is easy enough to implement sequentially, but in parallel this
requires some care.

Let's divide the grid $G$ and divide it over a two-dimension grid of
$p_i\times p_j$
processors. (Other strategies exist, but this one scales best; see
section~\HPSCref{sec:pspmvp}.)
Formally, we define two sequences of points
\[ 0=i_0<\cdots<i_{p_i}<i_{p_i+1}=n_i,\quad 
   0<j_0<\cdots<j_{p_j}<i_{p_j+1}=n_j
\]
and we say that processor $(p,q)$ computes $g_{ij}$ for
\[ i_p\leq i<i_{p+1}, \quad j_q\leq j<j_{q+1}.
\]
From formula~\eqref{eq:grid-update} you see that the processor then
needs one row of points on each side surrounding its part of the
grid. A~picture makes this clear; see figure~\ref{fig:ghost-grid}.
\begin{figure}
\includegraphics[scale=.1]{graphics/jacobi-average}
\caption{A grid divided over processors, with the `ghost' region indicated}
\label{fig:ghost-grid}
\end{figure}
These elements surrounding the processor's own part are called
the \indexterm{halo} or \indexterm{ghost region} of that processor.

The problem is now that the elements in the halo are stored on a
different processor, so communication is needed to gather them. In the
upcoming exercises you will have to use different strategies for doing
so.

\Level 2 {Code basics}

The program needs to read the values of the grid size and the
processor grid size from the commandline, as well as the number of
iterations. This routine does some error checking: if the number of
processors does not add up to the size of \n{MPI_COMM_WORLD}, a
nonzero error code is returned.
\begin{verbatim}
ierr = parameters_from_commandline
  (argc,argv,comm,&ni,&nj,&pi,&pj,&nit);
if (ierr) return MPI_Abort(comm,1);
\end{verbatim}
From the processor parameters we make a processor grid object:
\begin{verbatim}
processor_grid *pgrid = new processor_grid(comm,pi,pj);
\end{verbatim}
and from the numerical parameters we make a number grid:
\begin{verbatim}
number_grid *grid = new number_grid(pgrid,ni,nj);
\end{verbatim}
Number grids have a number of methods defined. To set the value of all
the elements belonging to a processor to that processor's number:
\begin{verbatim}
grid->set_test_values();
\end{verbatim}
To set random values:
\begin{verbatim}
grid->set_random_values();
\end{verbatim}
If you want to visualize the whole grid, the following call gathers
all values on processor zero and prints them:
\begin{verbatim}
grid->gather_and_print();
\end{verbatim}

Next we need to look at some data structure details.

The definition of the \n{number_grid} object starts as follows:
\begin{verbatim}
class number_grid {
public:
  processor_grid *pgrid;
  double *values,*shadow;
\end{verbatim}
where \n{values} contains the elements owned by the processor,
and \n{shadow} is intended to contain the values plus the ghost
region. So how does \n{shadow} receive those values? Well, the call looks 
like
\begin{verbatim}
grid->build_shadow();
\end{verbatim}
and you will need to supply the implementation of that.
Once you've done so, there is a routine that prints out the shadow array 
of each processor
\begin{verbatim}
grid->print_shadow();
\end{verbatim}
This routine does the sequenced printing that you implemented in
exercise~\ref{ex:hello-world-zero}.

In the file \n{code/mpi/c/grid_impl.cxx} you can see several uses of
the macro \n{INDEX}. This translates from a two-dimensional coordinate
system to one-dimensional. Its main use is letting you use $(i,j)$
coordinates for indexing the processor grid and the number grid: for
processors you need the translation to the linear rank, and for the
grid you need the translation to the linear array that holds the
values.

A good example of the use of \n{INDEX} is in the
\n{number_grid::relax} routine: this takes points from the \n{shadow}
array and averages them into a point of the \n{values} array. (To
understand the reason for this particular averaging,
see~\HPSCref{sec:2dbvp} and~\HPSCref{sec:jacobi-seidel}.) Note how the
\n{INDEX} macro is used to index in a
$\n{ilength}\times\n{jlength}$ target array \n{values}, while
reading from a $(\n{ilength}+2)\times(\n{jlength}+2)$ source array
\n{shadow}.
\begin{verbatim}
for (i=0; i<ilength; i++) {
  for (j=0; j<jlength; j++) {
    int c=0;
    double new_value=0.;
    for (c=0; c<5; c++) {
	int ioff=i+1+ioffsets[c],joff=j+1+joffsets[c];
	new_value += coefficients[c] * 
	  shadow[ INDEX(ioff,joff,ilength+2,jlength+2) ];
    }
    values[ INDEX(i,j,ilength,jlength) ] = new_value/8.;
  }
}
\end{verbatim}

