% -*- latex -*-
\Level 0 {Warmpup exercises}

We start with some simple exercises.

\Level 1 {Hello world}

First of all we need to make sure that you have a working setup for
parallel jobs. The example program \n{helloworld.c} does the
following:
\verbatimsnippet{hello}
Compile this program and run it in parallel. Make sure that the processors
do \emph{not} all say that they are \n{processor 0 out of 1}!

\begin{istc}
  We want to make trace files of the parallel runs, for which we'll
  use the TAU utility of the University of Oregon. Here are the steps:
  \begin{itemize}
  \item Load two modules:
\begin{verbatim}
module load tau
module load jdk64
\end{verbatim}
  \item Recompile your program with \n{make yourprog}. You'll notice a
    lot more output: that is the TAU preprocessor.
  \item Now run your program, setting environment variables
    \n{TAU_TRACE} and \n{TAU_PROFILE} to~1, and \n{TRACEDIR} and
    \n{PROFILEDIR} to where you want the output to be.  Big shortcut:
    do 
\begin{verbatim}
make submit EXECUTABLE=yourprog
\end{verbatim}
    for a batch job or 
\begin{verbatim}
make idevrun EXECUTABLE=yourprog
\end{verbatim}
    for an interactive parallel
    run. These last two set all variables for you. See if you can find
    where the output went\ldots
  \item Now you need to postprocess the TAU output. Do \n{make tau
    EXECUTABLE=yourprog} and you'll get a file
    \n{taulog_yourprog.slog2} which you can view with the \n{jumpshot}
    program.
  \end{itemize}
\end{istc}

\Level 1 {Collectives}

It is a good idea to be able to collect statistics, so before we do
anything interesting, we will look at MPI collectives;
section~\ref{sec:collective}.

Take a look at \n{time_max.cxx}. This program sleeps for a random
number of seconds: 
\verbatimsnippet{makejitter}
and measures how long the sleep actually was:
\verbatimsnippet{measurejitter}
In the code, this quantity is called `jitter', which is a term
for random deviations in a system.

\begin{exercise}
  Change this program to compute the average jitter by changing the reduction
  operator.
\end{exercise}

\begin{exercise}
  Now compute the standard deviation. For this you need to broadcast
  the average back to the processor, and do a second reduction. (You
  can also change the reduction to an \n{Allreduce}.
\end{exercise}

\begin{exercise}
  Finally, use a gather call to collect all the values on processor zero,
  and print them out.
\end{exercise}

\begin{istc}
\begin{exercise}
  Make a TAU trace of these programs. Can you see what algorithms is
  used for the broadcast and reduction?
\end{exercise}
\end{istc}

\Level 1 {Linear arrays of processors}

Next let us consider an operation that should be very parallel: all
processors pass a data item to the processor with the next higher
number. However, if we code this naively, we get an unexpected serial
behaviour:
\verbatimsnippet{linear}

\begin{exercise}
  Compile and run this code, and generate a TAU trace file. Confirm
  that the execution is serial.
\end{exercise}
First we clean up the code a little.
\begin{exercise}
  First write this code more elegantly by using \n{MPI_PROC_NULL}.
\end{exercise}
Next we try to fix the problem.
\begin{exercise}
  Rewrite the code using \n{MPI_Sendrecv}. Confirm with a TAU trace
  that execution is no longer serial.
\end{exercise}
There are other ways of fixing the code.
\begin{exercise}
  Implement a fully parallel version by using \n{MPI_Isend} and
  \n{MPI_Irecv}.
\end{exercise}

\Level 0 {Mandelbrot set}

If you've never heard the name \indexterm{Mandelbrot set}, you
probably recognize the picture. Its formal definition is as follows:
\begin{quotation}\noindent
  A point $c$ in the complex plane is part of the Mandelbrot set if 
  the series $x_n$ defined by 
  \[ 
  \begin{cases}
    x_0=0\\ x_{n+1}=x_n^2+c
  \end{cases}
  \] satisfies \[ \forall_n\colon |x_n|\leq 2. \]  
\end{quotation}
It is easy to see that only points $c$ in the bounding circle
$|c|< 2$ qualify, but
apart from that it's hard to say much without a lot more thinking.
Or computing; and that's what we're going to do.

The way we approach this, is as a \indexterm{master-worker} model: 
there is one master processor which gives out work to, and accepts
results from, the worker processors.

\Level 1 {Tools}

The driver part of the Mandelbrot program is simple. There is a 
circle object that can generate coordinates
\verbatimsnippet{circledef}
and a global routine that tests whether a coordinate is in the set,
at least up to an iteration bound:
\verbatimsnippet{belongs}
We use a fairly simple code for the worker processes: they 
execute a loop in which they wait 
for input, process it, return the result.
\verbatimsnippet{waitforwork}

A very simple solution using blocking sends on the master is given:
\verbatimsnippet{serialaddtask}

\begin{exercise}
  Explain why this solution is very inefficient. Make a trace of its
  execution that bears this out.
\end{exercise}

\Level 1 {Bulk task scheduling}
\label{proj:mandel-bulk}

The previous section showed a very inefficient solution, but that was mostly
intended to set up the code base. If all tasks take about the same amount of time,
you can give each process a task, and then wait on them all to finish. A~first way
to do this is with non-blocking sends.

\begin{exercise}
  Code a  solution where you give a task to all worker processes
  using non-blocking sends and receives, and then wait for these tasks
  with \n{MPI_Waitall}
  to finish before you give a new round of data to all workers.
  Make a trace of the execution of this and report on the total time.

  You can do this by writing a new class that inherits from \n{queue},
  and that provides its own \n{addtask} method:
\verbatimsnippet{bulkq}
  You will also have to override the \n{complete} method: when the circle 
  object indicates that all coordinates have been generated, not all
  workers will be busy, so you need to supply the proper \n{MPI_Waitall}
  call.
\end{exercise}

\Level 1 {Collective task scheduling}
\label{proj:mandel-collective}

Another implementation of the bulk scheduling of the previous section
would be through using collectives.

\Level 1 {Asynchronous task scheduling}

At the start of section~\ref{proj:mandel-bulk} we said that bulk scheduling
mostly makes sense if all tasks take similar time to complete.
In the Mandelbrot case this is clearly not the case.

\begin{exercise}
  Code a fully dynamic solution that uses \n{MPI_Probe} or \n{MPI_Waitany}.
  Make an execution trace and report on the total running time.
\end{exercise}

\Level 0 {Data parallel grids}

\Level 1 {A realistic programming example}

In this section we will gradually build a semi-realistic example
program. To get you started some pieces have already been written:
as a starting point look at \n{code/mpi/c/grid.cxx}.

\Level 2 {Description of the problem}

With this example you will investigate several strategies for
implementing a simple iterative method. Let's say you have a
two-dimensional grid of datapoints $G=\{g_{ij}\colon 0\leq
i<n_i,\,0\leq j<n_j\}$ and you want to compute~$G'$ where
\begin{equation}
g'_{ij} = 1/4 \cdot (g_{i+1,j}+g_{i-1,j}+g_{i,j+1}+g_{i,j-1}).
\label{eq:grid-update}
\end{equation}

This is easy enough to implement sequentially, but in parallel this
requires some care.

Let's divide the grid $G$ and divide it over a two-dimension grid of
$p_i\times p_j$
processors. (Other strategies exist, but this one scales best; see
section~\HPSCref{sec:pspmvp}.)
Formally, we define two sequences of points
\[ 0=i_0<\cdots<i_{p_i}<i_{p_i+1}=n_i,\quad 
   0<j_0<\cdots<j_{p_j}<i_{p_j+1}=n_j
\]
and we say that processor $(p,q)$ computes $g_{ij}$ for
\[ i_p\leq i<i_{p+1}, \quad j_q\leq j<j_{q+1}.
\]
From formula~\eqref{eq:grid-update} you see that the processor then
needs one row of points on each side surrounding its part of the
grid. A~picture makes this clear; see figure~\ref{fig:ghost-grid}.
\begin{figure}
\includegraphics[scale=.1]{graphics-public/jacobi-average}
\caption{A grid divided over processors, with the `ghost' region indicated}
\label{fig:ghost-grid}
\end{figure}
These elements surrounding the processor's own part are called
the \indexterm{halo} or \indexterm{ghost region} of that processor.

The problem is now that the elements in the halo are stored on a
different processor, so communication is needed to gather them. In the
upcoming exercises you will have to use different strategies for doing
so.

\Level 2 {Code basics}

The program needs to read the values of the grid size and the
processor grid size from the commandline, as well as the number of
iterations. This routine does some error checking: if the number of
processors does not add up to the size of \n{MPI_COMM_WORLD}, a
nonzero error code is returned.
\begin{verbatim}
ierr = parameters_from_commandline
  (argc,argv,comm,&ni,&nj,&pi,&pj,&nit);
if (ierr) return MPI_Abort(comm,1);
\end{verbatim}
From the processor parameters we make a processor grid object:
\begin{verbatim}
processor_grid *pgrid = new processor_grid(comm,pi,pj);
\end{verbatim}
and from the numerical parameters we make a number grid:
\begin{verbatim}
number_grid *grid = new number_grid(pgrid,ni,nj);
\end{verbatim}
Number grids have a number of methods defined. To set the value of all
the elements belonging to a processor to that processor's number:
\begin{verbatim}
grid->set_test_values();
\end{verbatim}
To set random values:
\begin{verbatim}
grid->set_random_values();
\end{verbatim}
If you want to visualize the whole grid, the following call gathers
all values on processor zero and prints them:
\begin{verbatim}
grid->gather_and_print();
\end{verbatim}

Next we need to look at some data structure details.

The definition of the \n{number_grid} object starts as follows:
\begin{verbatim}
class number_grid {
public:
  processor_grid *pgrid;
  double *values,*shadow;
\end{verbatim}
where \n{values} contains the elements owned by the processor,
and \n{shadow} is intended to contain the values plus the ghost
region. So how does \n{shadow} receive those values? Well, the call looks 
like
\begin{verbatim}
grid->build_shadow();
\end{verbatim}
and you will need to supply the implementation of that.
Once you've done so, there is a routine that prints out the shadow array 
of each processor
\begin{verbatim}
grid->print_shadow();
\end{verbatim}
This routine does the sequenced printing that you implemented in
exercise~\ref{ex:hello-world-zero}.

In the file \n{code/mpi/c/grid_impl.cxx} you can see several uses of
the macro \n{INDEX}. This translates from a two-dimensional coordinate
system to one-dimensional. Its main use is letting you use $(i,j)$
coordinates for indexing the processor grid and the number grid: for
processors you need the translation to the linear rank, and for the
grid you need the translation to the linear array that holds the
values.

A good example of the use of \n{INDEX} is in the
\n{number_grid::relax} routine: this takes points from the \n{shadow}
array and averages them into a point of the \n{values} array. (To
understand the reason for this particular averaging,
see~\HPSCref{sec:2dbvp} and~\HPSCref{sec:jacobi-seidel}.) Note how the
\n{INDEX} macro is used to index in a
$\n{ilength}\times\n{jlength}$ target array \n{values}, while
reading from a $(\n{ilength}+2)\times(\n{jlength}+2)$ source array
\n{shadow}.
\begin{verbatim}
for (i=0; i<ilength; i++) {
  for (j=0; j<jlength; j++) {
    int c=0;
    double new_value=0.;
    for (c=0; c<5; c++) {
	int ioff=i+1+ioffsets[c],joff=j+1+joffsets[c];
	new_value += coefficients[c] * 
	  shadow[ INDEX(ioff,joff,ilength+2,jlength+2) ];
    }
    values[ INDEX(i,j,ilength,jlength) ] = new_value/8.;
  }
}
\end{verbatim}

