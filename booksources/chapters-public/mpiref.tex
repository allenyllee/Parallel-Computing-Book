This section gives reference information and illustrative examples
of the use of MPI. While the code snippets given here should be enough,
full programs can be found in the repository for this book
\url{https://bitbucket.org/VictorEijkhout/parallel-computing-book}.

\Level 1 {Basics}

\Level 2 {MPI setup}
\commandreflabel{mpi-init}

Every MPI program has to start with\indexmpi{MPI_Init}
\begin{verbatim}
MPI_Init(&argc,&argv);
\end{verbatim}
where \indextermtt{argc} and \indextermtt{argv} are the arguments
of a C language main program:
\begin{verbatim}
int main(int argc,char **argv) {
    ....
    return 0;
}
\end{verbatim}
The regular way to conclude an MPI program is through\indexmpi{MPI_Finalize}
\begin{verbatim}
MPI_Finalize();
\end{verbatim}
but an abnormal end to a run can be forced by\indexmpi{MPI_Abort}
\begin{verbatim}
MPI_Abort(comm,value);
\end{verbatim}
This aborts execution on all processes associated with the communicator,
but many implementations simply abort all processes. The \n{value} parameter
is returned to the environment.

The commandline arguments can only be guaranteed to be passed correctly to 
process zero. Here is a fragment of code that shows use of commandline arguments.
The program \n{examples-public/mpi/c/init.c} takes a single integer commandline argument.
If the user forgets to specify an argument of specifies~\n{-h}, a usage message
is printed and the program aborts, otherwise the parameter is broadcast to 
all processes.
\verbatimsnippet{usage}

\Level 2 {MPI ranks and communicator sizes}
\commandreflabel{rank-size}

Every MPI process has its own local storage. So if you pretend that all these
small arrays are really together one big array, you need to know where each 
piece fits in the whole. For this you need to know at the very least
how many processes there are and what the rank of a process is.

The following example creates a distributed array that will contains
the values of a function at equidistant points in the interval~$[0,1]$.
The code implements this as follows:
\begin{enumerate}
\item each process has an array of 10 points,
\item so the distributed array has a length of 10 times the number of processes;
\item of this array, each process has 10 points, starting at 10 times
  the process id.
\item To fill in values in the local array, the process iterates only
  over its local points,
\item but it needs to calculate the global coordinate of those points.
\end{enumerate}
 
\verbatimsnippet{local}

\Level 2 {Send and receive buffers}

The data is specified as a number of elements in a buffer. The same
MPI routine can be used with data of different types, so the standard
indicates such buffers as \indexterm{choice}. The specification of
this differs per language:
\begin{itemize}
\item In C it is an address, so the clean way is to pass it as
  \verb+(void*)&myvar+.
\item Fortran compilers may complain about type mismatches. This can
  not be helped.
\end{itemize}

\Level 2 {Types}

\n{double} vs \n{MPI_DOUBLE}, Fortran especially.

Addresses have type \indexmpishow{MPI_Aint} or \n{INTEGER
(KIND=MPI_ADDRESS_KIND)} in Fortran. The start of the address range is
given in \indexmpishow{MPI_BOTTOM}.

\Level 1 {Blocking communication}
\commandreflabel{blocking}

The basic send command is
\begin{verbatim}
int MPI_Send(void *buf, 
  int count, MPI_Datatype datatype, int dest, int tag,
  MPI_Comm comm)
\end{verbatim}
\url{http://www.mcs.anl.gov/research/projects-public/mpi/www/www3/MPI_Send.html}
This routine may not blocking for small messages; to force blocking
behaviour use \n{MPI_Ssend} with the same argument list.
\url{http://www.mcs.anl.gov/research/projects-public/mpi/www/www3/MPI_Ssend.html}

The basic blocking receive command is
\begin{verbatim}
int MPI_Recv(void *buf, 
  int count, MPI_Datatype datatype, int source, int tag,
  MPI_Comm comm, MPI_Status *status)
\end{verbatim}
\url{http://www.mcs.anl.gov/research/projects-public/mpi/www/www3/MPI_Recv.html}
The \n{count} argument indicates the maximum length of a message; the
actual length of the message can be determined with \n{MPI_Get_count}; 
see section~\ref{sec:mpi-source}.

The following code is guaranteed to block, since a \n{MPI_Recv}
always blocks:
\verbatimsnippet{recvblock}
On the other hand, if we put the send call before the receive,
code may not block for small messages
that fall under the \indexterm{eager limit}. In this example we send
gradually larger messages. From the screen output you can see what
the largest message was that fell under the eager limit; after that the code
hangs because of a deadlock.
\verbatimsnippet{sendblock}
If you want a code to behave the same for all message sizes,
you force the send call to be blocking by using \n{MPI_Ssend}:
\verbatimsnippet{ssendblock}

\Level 2 {Receive status}
\commandreflabel{mpi-source}

In section~\ref{sec:mpi-source} we mentioned the master-worker model
as one opportunity for inspecting the \indexmpishow{MPI_SOURCE} field
of the \indexmpishow{MPI_Status} object. Here is a small example: 
the tasks perform a variable amount of work (modeled here by a random wait)
before sending a message to the master. The master waits for any source,
and inspects the status field to report where the message comes from.

\verbatimsnippet{anysource}

\Level 1 {Deadlock-free blocking messages}
\commandreflabel{send-recv}

If messsages are send roughly in pairs, the 
\indexmpishow{MPI_Sendrecv} call is an easy way to prevent deadlock.
Here you specify both the target of a send and the source of a receive,
which can be same in case of a pairwise exchange of data,
but they need not be the same.
\begin{verbatim}
int MPI_Sendrecv(
  void *sendbuf, int sendcount, MPI_Datatype sendtype, 
                int dest, int sendtag,
  void *recvbuf, int recvcount, MPI_Datatype recvtype, 
                int source, int recvtag,
  MPI_Comm comm, MPI_Status *status)
\end{verbatim}

As an example we set up a ring of three processors: each process sends
to its right neighbour, and receives from its left neighbour.
\verbatimsnippet{sendrecvring}

\Level 1 {Non-blocking communication}
\commandreflabel{nonblocking}

The non-blocking routines have much the same parameter list as the 
blocking ones, with the addition of an \n{MPI_Request} parameter.
The \n{MPI_Isend} routine does not have a `status' parameter,
which has moved to the `wait' routine.
\begin{verbatim}
int MPI_Isend(void *buf,
  int count, MPI_Datatype datatype, int dest, int tag,
  MPI_Comm comm, MPI_Request *request)
\end{verbatim}
\url{http://www.mcs.anl.gov/research/projects-public/mpi/www/www3/MPI_Isend.html}
\begin{verbatim}
int MPI_Irecv(void *buf,
  int count, MPI_Datatype datatype, int source, int tag,
  MPI_Comm comm, MPI_Request *request)
\end{verbatim}
\url{http://www.mcs.anl.gov/research/projects-public/mpi/www/www3/MPI_Irecv.html}

There are various `wait' routines. Since you will often do at least
one send and one receive, this routine is useful:
\begin{verbatim}
int MPI_Waitall(int count, MPI_Request array_of_requests[], 
  MPI_Status array_of_statuses[])
\end{verbatim}
\url{http://www.mcs.anl.gov/research/projects-public/mpi/www/www3/MPI_Waitall.html}

Here is a simple code that does a non-blocking exchange between two processors:
\verbatimsnippet{irecvnonblock}

It is possible to omit the status array by specifying \n{MPI_STATUSES_IGNORE}.
Other routines are \n{MPI_Wait} for a single request, and
\n{MPI_Waitsome}, \n{MPI_Waitany}.

The above fragment is unrealistically simple. In a more general scenario we
have to manage send and receive buffers: we need as many buffers as there are
simultaneous non-blocking sends and receives.
\verbatimsnippet{irecvloop}

Instead of waiting for all messages, we can wait for any message to come
with \indexmpishow{MPI_Waitany}, and process the receive data as it comes in.
\verbatimsnippet{waitforany}
Note the \indexmpishow{MPI_STATUS_IGNORE} parameter: we know everything
about the incoming message, so we do not need to query a status object.
Contrast this with the example in section~\ref{ref:mpi-source}.

\Level 1 {One-sided communication}
\commandreflabel{one-sided}

\Level 2 {Windows and epochs}
\commandreflabel{windows}

\begin{verbatim}
MPI_WIN_CREATE (void *base, MPI_Aint size, 
  int disp_unit, MPI_Info info, 
  MPI_Comm comm, MPI_Win *win, ierr)
\end{verbatim}
The data array must not be \n{PARAMETER} of \n{static const}.

Here is a single put operation. Note that the window create and window fence calls
are collective, so they have to be performed on all processors
of the communicator that was used in the create call.
\verbatimsnippet{putblock}

The \indexmpishow{MPI_Info} parameter can be used to pass implementation-dependent 
information:
\begin{verbatim}
MPI_Info info;
MPI_Info_create(&info);
MPI_Info_set(info,"no_locks","true");
MPI_Win_create( ... info ... &win);
MPI_Info_free(&info);
\end{verbatim}
It is always valid to use \indexmpishow{MPI_INFO_NULL}.

\Level 2 {More active target synchronization}
\commandreflabel{post-wait}

You start and complete an \indextermsub{exposure}{epoch} with%
\indexmpi{MPI_Win_post}\indexmpi{MPI_Win_wait}:
\begin{verbatim}
int MPI_Win_post(MPI_Group group, int assert, MPI_Win win)
int MPI_Win_wait(MPI_Win win)
\end{verbatim}
In other words, this turns your window into the \indexterm{target} for a remote access.

You start and complete an \indextermsub{access}{epoch} with%
\indexmpi{MPI_Win_start}\indexmpi{MPI_Win_complete}:
\begin{verbatim}
int MPI_Win_start(MPI_Group group, int assert, MPI_Win win)
int MPI_Win_complete(MPI_Win win)
\end{verbatim}
In other words, these calls border the access to a remote window, with the current processor
being the \indexterm{origin} of the remote access.

In the following snippet a single processor puts data on one
other. Note that they both have their own definition of the group, and
that the receiving process only does the post and wait calls.
\verbatimsnippet{postwaittwo}

\Level 1 {Collectives}
\commandreflabel{collective}

\begin{verbatim}
int MPI_Bcast( void *buffer, int count, MPI_Datatype datatype, int root, 
               MPI_Comm comm )
\end{verbatim}

\begin{verbatim}
int MPI_Reduce(void *sendbuf, void *recvbuf, int count, MPI_Datatype datatype, 
               MPI_Op op, int root, MPI_Comm comm)
\end{verbatim}
On processes that are not the root, the receive buffer is ignored. On the root, 
you have two buffers, but by specifying \indexmpishow{MPI_IN_PLACE}, the reduction call
uses the value in the receive buffer as the root's contribution to the operation.
On the \n{Allreduce} call, \n{MPI_IN_PLACE} can be used for the send buffer of
every process.

The scan operations are
\begin{verbatim}
int MPI_Scan(void* sendbuf, void* recvbuf, 
    int count, MPI_Datatype datatype, MPI_Op op, MPI_Comm comm ) 
\end{verbatim}
and
\begin{verbatim}
int MPI_Exscan(void* sendbuf, void* recvbuf, 
    int count, MPI_Datatype datatype, MPI_Op op, MPI_Comm comm ) 
\end{verbatim}
The \n{MPI_Op} operations do not return an error code.

The result of the exclusive scan is undefined on processor~0,
and on processor~1 it is a copy of the send value of processor~1.
In particular, the \n{MPI_Op} need not be called on these two 
processors.

\Level 1 {Cancelling messages}

In section~\ref{sec:mpi-source} we showed a master-worker example where the 
master accepts in arbitrary order the messages from the workers.
Here we will show a slightly
more complicated example, where only the result of the first task to
complete is needed. Thus, we issue an \n{MPI_Recv}
with \indexmpishow{MPI_ANY_SOURCE} as source.  When a result comes, we
broadcast its source to all processes.  All the other workers then use
this information to cancel their message with
an \indexmpishow{MPI_Cancel} operation.

\verbatimsnippet{cancel}

\Level 1 {Communicators}

\Level 2 {Communicator duplication}
\commandreflabel{comm-dup}

In section~\ref{sec:mpi-semantics} it was explained that MPI messages are 
non-overtaking. This may lead to confusing situations, witness the following snippet:

\verbatimsnippet{wrongcatchmain}

This models a main program that does a simple message exchange, and it
makes two calls to library routines. Unbeknown to the user, the library also issues send and receive calls, and they turn out to interfere:

\verbatimsnippet{wrongcatchlib}

Here
\begin{itemize}
\item The main program does a send,
\item the library call \n{function_start} does a send and a receive;
  because the receive can match either send, it is paired with the
  first one;
\item the main program does a receive, which will be paired with the send of the 
  library call;
\item both the main program and the library do a wait call, and in
  both cases all requests are succesfully fulfilled, just not the way
  you intended.
\end{itemize}

The solution is to give the library a separate communicator with
\indexmpishow{MPI_Comm_dup}. Newly created communicators should be
released again with \indexmpishow{MPI_Comm_free}.

\verbatimsnippet{rightcatchmain}

\Level 2 {Splitting communicators}
\commandreflabel{comm-split}

The command \indexmpishow{MPI_Comm_split} takes a communicator, and
divides it into a number of disjoint communicators. It does this by
assigning processes to the same subcommunicator if they have the same
user-specified `colour' value.
\begin{verbatim}
int MPI_Comm_split(MPI_Comm comm, int color, int key, 
                   MPI_Comm *newcomm)
\end{verbatim}
The ranking of processes in the new communicator is determined by a `key' value.
Most of the time, there is no reason to use a relative ranking that is different from
the global ranking, so the \n{MPI_Comm_rank} value of the global communicator
is a good choice.
\verbatimsnippet{commsplitrowcol}

\Level 1 {Error handling}
\commandreflabel{mpi:error}

MPI operators (\indexmpishow{MPI_Op}) do not return an error code. In case of
an error they call \n{MPI_Abort}; if \indexmpishow{MPI_ERRORS_RETURN}
is the error handler, errors may be silently ignore.

\Level 1 {More utility stuff}
\commandreflabel{profile}

MPI has a \indexterm{wall clock} timer: \indexmpishow{MPI_Wtime}
\begin{verbatim}
double MPI_Wtime(void);
\end{verbatim}
which gives the number of seconds from a certain point in the past.
Thus, you would write:
\begin{verbatim}
double tstart,tstop,elapsed;
tstart = MPI_Wtime();
tstop = MPI_Wtime();
elapsed = tstop-tstart;
\end{verbatim}
The timer has a resolution of \indexmpishow{MPI_Wtick}:
\begin{verbatim}
double MPI_Wtick(void);
\end{verbatim}
Timing in parallel is a tricky issue. For instance, most clusters do
not have a central clock, so you can not relate start and stop times
on one process to those on another. You can test for a global clock as
follows\indexmpi{MPI_WTIME_IS_GLOBAL}:
\begin{verbatim}
int *v,flag;
MPI_Attr_get( comm, MPI_WTIME_IS_GLOBAL, &v, &flag );
if (mytid==0) printf(``Time synchronized? %d->%d\n'',flag,*v);
\end{verbatim}
%\indexmpi{MPI_Wtime} can be either a function or a macro.

\Level 1 {Multi-threading}
\commandreflabel{mpi-thread}

\begin{verbatim}
int MPI_Init_thread( int *argc, char ***argv, int required, int *provided )
\end{verbatim}

\begin{itemize}
\item \indexmpishow{MPI_THREAD_SINGLE}: each MPI process can only have
  a single thread.
\item \indexmpishow{MPI_THREAD_FUNNELED}: an MPI process can be
  multithreaded, but all MPI calls need to be done from a single
  thread.
\item \indexmpishow{MPI_THREAD_SERIALIZED}: a processes can sustain
  multiple threads that make MPI calls, but these threads can not be
  simultaneous: they need to be for instance in an OpenMP
  \indexterm{critical section}.
\item \indexmpishow{MPI_THREAD_MULTIPLE}: processes can be fully
  generally multi-threaded.
\end{itemize}
