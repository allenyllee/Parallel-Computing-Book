This section gives reference information and illustrative examples
of the use of MPI. While the code snippets given here should be enough,
full programs can be found in the repository for this book
\url{https://bitbucket.org/VictorEijkhout/parallel-computing-book}.

\Level 1 {Basics}

\Level 2 {MPI setup}
\commandreflabel{sec:mpi-init}

Every MPI program has to start with\indexmpi{MPI_Init}
\begin{verbatim}
MPI_Init(&argc,&argv);
\end{verbatim}
where \indextermtt{argc} and \indextermtt{argv} are the arguments
of a C language main program:
\begin{verbatim}
int main(int argc,char **argv) {
    ....
    return 0;
}
\end{verbatim}
The regular way to conclude an MPI program is through\indexmpi{MPI_Finalize}
\begin{verbatim}
MPI_Finalize();
\end{verbatim}
but an abnormal end to a run can be forced by\indexmpi{MPI_Abort}
\begin{verbatim}
MPI_Abort(comm,value);
\end{verbatim}
This aborts execution on all processes associated with the communicator,
but many implementations simply abort all processes. The \n{value} parameter
is returned to the environment.

The commandline arguments can only be guaranteed to be passed correctly to 
process zero. Here is a fragment of code that shows use of commandline arguments.
The program \n{examples-public/mpi/c/init.c} takes a single integer commandline argument.
If the user forgets to specify an argument of specifies~\n{-h}, a usage message
is printed and the program aborts, otherwise the parameter is broadcast to 
all processes.
\verbatimsnippet{usage}

\Level 2 {MPI ranks and communicator sizes}
\commandreflabel{sec:rank-size}

\Level 2 {Send and receive buffers}

The data is specified as a number of elements in a buffer. The same
MPI routine can be used with data of different types, so the standard
indicates such buffers as \indexterm{choice}. The specification of
this differs per language:
\begin{itemize}
\item In C it is an address, so the clean way is to pass it as
  \verb+(void*)&myvar+.
\item Fortran compilers may complain about type mismatches. This can
  not be helped.
\end{itemize}

\Level 2 {Types}

\n{double} vs \n{MPI_DOUBLE}, Fortran especially.

Addresses have type \indexmpishow{MPI_Aint} or \n{INTEGER
(KIND=MPI_ADDRESS_KIND)} in Fortran. The start of the address range is
given in \indexmpishow{MPI_BOTTOM}.

\Level 1 {Blocking communication}
\commandreflabel{sec:blocking}

The basic send command is
\begin{verbatim}
int MPI_Send(void *buf, 
  int count, MPI_Datatype datatype, int dest, int tag,
  MPI_Comm comm)
\end{verbatim}
\url{http://www.mcs.anl.gov/research/projects-public/mpi/www/www3/MPI_Send.html}
This routine may not blocking for small messages; to force blocking
behaviour use \n{MPI_Ssend} with the same argument list.
\url{http://www.mcs.anl.gov/research/projects-public/mpi/www/www3/MPI_Ssend.html}

The basic blocking receive command is
\begin{verbatim}
int MPI_Recv(void *buf, 
  int count, MPI_Datatype datatype, int source, int tag,
  MPI_Comm comm, MPI_Status *status)
\end{verbatim}
\url{http://www.mcs.anl.gov/research/projects-public/mpi/www/www3/MPI_Recv.html}
The \n{count} argument indicates the maximum length of a message; the
actual length of the message can be determined with \n{MPI_Get_count}.

The following code is guaranteed to block, since a \n{MPI_Recv}
always blocks:
\verbatimsnippet{recvblock}
On the other hand, this code may not block for small messages:
\verbatimsnippet{sendblock}
If you want to force uniform behaviour use \n{MPI_Ssend}:
\verbatimsnippet{ssendblock}

\Level 1 {Non-blocking communication}
\commandreflabel{sec:nonblocking}

The non-blocking routines have much the same parameter list as the 
blocking ones, with the addition of an \n{MPI_Request} parameter.
The \n{MPI_Isend} routine does not have a `status' parameter,
which has moved to the `wait' routine.
\begin{verbatim}
int MPI_Isend(void *buf,
  int count, MPI_Datatype datatype, int dest, int tag,
  MPI_Comm comm, MPI_Request *request)
\end{verbatim}
\url{http://www.mcs.anl.gov/research/projects-public/mpi/www/www3/MPI_Isend.html}
\begin{verbatim}
int MPI_Irecv(void *buf,
  int count, MPI_Datatype datatype, int source, int tag,
  MPI_Comm comm, MPI_Request *request)
\end{verbatim}
\url{http://www.mcs.anl.gov/research/projects-public/mpi/www/www3/MPI_Irecv.html}

There are various `wait' routines. Since you will often do at least
one send and one receive, this routine is useful:
\begin{verbatim}
int MPI_Waitall(int count, MPI_Request array_of_requests[], 
  MPI_Status array_of_statuses[])
\end{verbatim}
\url{http://www.mcs.anl.gov/research/projects-public/mpi/www/www3/MPI_Waitall.html}

Here is a simple code that does a non-blocking exchange between two processors:
\verbatimsnippet{irecvnonblock}

It is possible to omit the status array by specifying \n{MPI_STATUSES_IGNORE}.
Other routines are \n{MPI_Wait} for a single request, and
\n{MPI_Waitsome}, \n{MPI_Waitany}.

The above fragment is unrealistically simple. In a more general scenario we
have to manage send and receive buffers: we need as many buffers as there are
simultaneous non-blocking sends and receives.
\verbatimsnippet{irecvloop}

\Level 1 {One-sided communication}
\commandreflabel{sec:one-sided}

\Level 2 {Windows and epochs}
\commandreflabel{sec:windows}

\begin{verbatim}
MPI_WIN_CREATE (void *base, MPI_Aint size, 
  int disp_unit, MPI_Info info, 
  MPI_Comm comm, MPI_Win *win, ierr)
\end{verbatim}
The data array must not be \n{PARAMETER} of \n{static const}.

Here is a single put operation. Note that the window create and window fence calls
are collective, so they have to be performed on all processors.
\verbatimsnippet{putblock}

The \indexmpishow{MPI_Info} parameter can be used to pass implementation-dependent 
information:
\begin{verbatim}
MPI_Info info;
MPI_Info_create(&info);
MPI_Info_set(info,"no_locks","true");
MPI_Win_create( ... info ... &win);
MPI_Info_free(&info);
\end{verbatim}
It is always valid to use \indexmpishow{MPI_INFO_NULL}.

\Level 1 {Collectives}
\commandreflabel{sec:collective}

\begin{verbatim}
int MPI_Bcast( void *buffer, int count, MPI_Datatype datatype, int root, 
               MPI_Comm comm )
\end{verbatim}

\begin{verbatim}
int MPI_Reduce(void *sendbuf, void *recvbuf, int count, MPI_Datatype datatype, 
               MPI_Op op, int root, MPI_Comm comm)
\end{verbatim}
On processes that are not the root, the receive buffer is ignored. On the root, 
you have two buffers, but by specifying \indexmpishow{MPI_IN_PLACE}, the reduction call
uses the value in the receive buffer as the root's contribution to the operation.
On the \n{Allreduce} call, \n{MPI_IN_PLACE} can be used for the send buffer of
every process.

The scan operations are
\begin{verbatim}
int MPI_Scan(void* sendbuf, void* recvbuf, 
    int count, MPI_Datatype datatype, MPI_Op op, MPI_Comm comm ) 
\end{verbatim}
and
\begin{verbatim}
int MPI_Exscan(void* sendbuf, void* recvbuf, 
    int count, MPI_Datatype datatype, MPI_Op op, MPI_Comm comm ) 
\end{verbatim}
The \n{MPI_Op} operations do not return an error code.

The result of the exclusive scan is undefined on processor~0,
and on processor~1 it is a copy of the send value of processor~1.
In particular, the \n{MPI_Op} need not be called on these two 
processors.

\Level 1 {Error handling}
\commandreflabel{mpi:error}

MPI operators (\indexmpishow{MPI_Op}) do not return an error code. In case of
an error they call \n{MPI_Abort}; if \indexmpishow{MPI_ERRORS_RETURN}
is the error handler, errors may be silently ignore.

\Level 1 {More utility stuff}
\commandreflabel{sec:profile}

MPI has a \indexterm{wall clock} timer: \indexmpishow{MPI_Wtime}
\begin{verbatim}
double MPI_Wtime(void);
\end{verbatim}
which gives the number of seconds from a certain point in the past.
Thus, you would write:
\begin{verbatim}
double tstart,tstop,elapsed;
tstart = MPI_Wtime();
tstop = MPI_Wtime();
elapsed = tstop-tstart;
\end{verbatim}
The timer has a resolution of \indexmpishow{MPI_Wtick}:
\begin{verbatim}
double MPI_Wtick(void);
\end{verbatim}
Timing in parallel is a tricky issue. For instance, most clusters do
not have a central clock, so you can not relate start and stop times
on one process to those on another. You can test for a global clock as
follows\indexmpi{MPI_WTIME_IS_GLOBAL}:
\begin{verbatim}
int *v,flag;
MPI_Attr_get( comm, MPI_WTIME_IS_GLOBAL, &v, &flag );
if (mytid==0) printf(``Time synchronized? %d->%d\n'',flag,*v);
\end{verbatim}
%\indexmpi{MPI_Wtime} can be either a function or a macro.

\Level 1 {Multi-threading}
\commandreflabel{sec:mpi-thread}

\begin{verbatim}
int MPI_Init_thread( int *argc, char ***argv, int required, int *provided )
\end{verbatim}

\begin{itemize}
\item \indexmpishow{MPI_THREAD_SINGLE}: each MPI process can only have
  a single thread.
\item \indexmpishow{MPI_THREAD_FUNNELED}: an MPI process can be
  multithreaded, but all MPI calls need to be done from a single
  thread.
\item \indexmpishow{MPI_THREAD_SERIALIZED}: a processes can sustain
  multiple threads that make MPI calls, but these threads can not be
  simultaneous: they need to be for instance in an OpenMP
  \indexterm{critical section}.
\item \indexmpishow{MPI_THREAD_MULTIPLE}: processes can be fully
  generally multi-threaded.
\end{itemize}
