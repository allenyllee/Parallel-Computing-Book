% -*- latex
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%
%%%% This TeX file is part of the tutorial
%%%% `Introduction to the PETSc library'
%%%% part of the book
%%%% Parallel Programming in Computational Science
%%%% Victor Eijkhout, eijkhout@tacc.utexas.edu
%%%% copyright Victor Eijkhout 2012-6
%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\sectionframe{The PETSc worldview: SPMD}

\frame{\frametitle{Computers when MPI was designed}
  \includegraphics[scale=.1]{mpi-node1}

  One processor and one  process per node;\\
  all communication goes through the network.
}

\frame{\frametitle{Pure MPI}
  \includegraphics[scale=.1]{mpi-node2}

  A node has multiple sockets, each with multiple cores.\\
  Pure MPI puts a process on each core: pretend shared memory doesn't exist.
}

\frame{\frametitle{Hybrid programming}
  \includegraphics[scale=.1]{mpi-node3}

  Hybrid programming puts a process per node or per socket;\\
  further parallelism comes from threading.\\
  Not in this course\ldots
}

\begin{frame}{Terminology}
  `Processor' is ambiguous: is that a chip or one independent
  instruction processing unit?
  \begin{itemize}
  \item Socket: the processor chip
  \item Processor: we don't use that word
  \item Core: one instruction-stream processing unit
  \item Process: preferred terminology in talking about MPI.
  \end{itemize}  
\end{frame}

\begin{frame}{Cluster setup}
  \small
  Typical cluster:
  \begin{itemize}
  \item Login nodes, where you ssh into; usually shared with 100 (or
    so) other people. You don't run your parallel program there!
  \item Compute nodes: where your job is run. They are often exclusive
    to you: no other users getting in the way of your program.
  \end{itemize}
  \includegraphics[scale=.08]{mpi-interactive}

  Hostfile: the description of where your job runs. Usually generated
  by a {job scheduler} such as SLURM.
\end{frame}

\begin{frame}[containsverbatim]{Do I need a supercomputer?}
  \begin{itemize}
  \item With \n{mpiexec} and such, you start a bunch of processes that
    execute your MPI program.
  \item Does that mean that you need a cluster or a big multicore?
  \item No! You can start a large number of MPI processes, even on
    your laptop. The OS will use `time slicing'.
  \item Of course it will not be very efficient\ldots
  \end{itemize}
\end{frame}

\frame{\frametitle{SPMD}
  The basic model of MPI is\\
  `Single Program Multiple Data':\\
  each process is an instance of the same program.

  Symmetry: There is no `master process', all processes are equal,
  start and end
  at the same time.

  Communication calls do not see the cluster structure:\\
  data sending/receiving is the same for all neighbours.
}

\begin{frame}{In a picture}
  \includegraphics[scale=.45]{hello-parallel}
\end{frame}

\begin{frame}{In a picture}
  Four processes on two nodes (\n{idev -N 2 -n 4})
  \includegraphics[scale=.3]{node-comm-rank}
\end{frame}

\begin{frame}[containsverbatim]{Sequential semantics}
  A PETSc program almost looks like sequential:
\begin{lstlisting}
MatMult(A,x,y);      // y <- Ax
VecCopy(y,res);      // r <- y
VecAXPY(res,-1.,b);  // r <- r - b
\end{lstlisting}
but note all objects are distributed.  
\end{frame}

\begin{frame}[containsverbatim]\frametitle{Minimal MPI knowledge}
  With PETSc you barely need to know MPI.\\
  Here is just about the only thing:
\begin{verbatim}
int MPI_Comm_rank( MPI_Comm comm, int *procno )
int MPI_Comm_size( MPI_Comm comm, int *nprocs )
\end{verbatim}
For now, the communicator will be \n{MPI_COMM_WORLD}.
\end{frame}
