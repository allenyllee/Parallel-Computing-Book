% -*- latex -*-
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%
%%%% This TeX file is part of the tutorial
%%%% `Introduction to the PETSc library'
%%%% by Victor Eijkhout, eijkhout@tacc.utexas.edu
%%%%
%%%% copyright Victor Eijkhout 2012-7
%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\sectionframe{\texttt{Mat} Datatype: matrix}

%\subsection{Matrix basics}

\frame[containsverbatim]{\frametitle{Matrix creation}
\small
The usual create/destroy calls:
\begin{verbatim}
MatCreate(MPI_Comm comm,Mat *A)
MatDestroy(Mat *A)
\end{verbatim}
Several more aspects to creation:
\begin{verbatim}
MatSetType(A,MATSEQAIJ) /* or MATMPIAIJ or MATAIJ */
MatSetSizes(Mat A,int m,int n,int M,int N)
MatSeqAIJSetPreallocation /* more about this later*/
  (Mat B,PetscInt nz,const PetscInt nnz[])
\end{verbatim}
Local or global size can be \n{PETSC_DECIDE} (as in the vector case)
}

\frame[containsverbatim]{\frametitle{If you already have a CRS matrix}
\begin{verbatim}
PetscErrorCode  MatCreateSeqAIJWithArrays
  (MPI_Comm comm,PetscInt m,PetscInt n,
   PetscInt* i,PetscInt*j,PetscScalar *a,Mat *mat)
\end{verbatim}
(also from triplets)

Do not use this unless you interface to a legacy code. And even then\ldots
}

\frame[containsverbatim]{\frametitle{Matrix Preallocation}

  \begin{itemize}
  \item PETSc matrix creation is very flexible:
  \item No preset sparsity pattern
  \item any processor can set any element\\
    $\Rightarrow$ potential for lots of malloc calls
  \item tell PETSc the matrix' sparsity structure\\ (do construction
    loop twice: once counting, once making)
  \item Re-allocating is expensive:
\begin{verbatim}
MatSetOption(A,MAT_NEW_NONZERO_LOCATIONS,PETSC_FALSE);
\end{verbatim}
  (is default) Otherwise:
\begin{verbatim}
  [1]PETSC ERROR: Argument out of range
  [1]PETSC ERROR: New nonzero at (0,1) caused a malloc
\end{verbatim}
  \end{itemize}

}

\frame[containsverbatim]{\frametitle{Sequential matrix  structure}
\begin{verbatim}
MatSeqAIJSetPreallocation
    (Mat B,PetscInt nz,const PetscInt nnz[])
\end{verbatim}
  \begin{itemize}
  \item \n{nz} number of nonzeros per row\\ (or slight overestimate)
  \item \n{nnz} array of row lengths (or overestimate)
  \item considerable savings over dynamic allocation!
  \end{itemize}
In Fortran use \n{PETSC_NULL_INTEGER} if not specifying \n{nnz} array
}

\frame[containsverbatim]{\frametitle{Parallel matrix structure}
\includegraphics[scale=.5]{petscmat}
}

\frame{\frametitle{(why does it do this?)}
  \begin{itemize}
  \item $y\leftarrow Ax_A+Bx_b$
  \item $x_B$ needs to be communicated; $Ax_A$ can be computed in the
    meantime
  \item Algorithm
    \begin{itemize}
    \item Initiate asynchronous sends/receives for $x_b$
    \item compute $Ax_A$
    \item make sure $x_b$ is in
    \item compute $Bx_B$
    \end{itemize}
  \item so by splitting matrix storage into $A,B$ part, code for the
    sequential case can be reused.
  \item This is one of the few places where PETSc's design is visible to
    the user.
  \end{itemize}
}

\frame[containsverbatim]{\frametitle{Parallel matrix structure description}
  \begin{itemize}
  \item \n{m,n} local size; \n{M,N} global. Note: If the matrix is
    square, specify $m,n$ equal, even though distribution by block rows
  \item \n{d_nz}: number of nonzeros per row in diagonal part
  \item \n{o_nz}: number of nonzeros per row in off-diagonal part
  \item \n{d_nnz}: array of numbers of nonzeros per row in diagonal part
  \item \n{o_nnz}: array of numbers of nonzeros per row in off-diagonal part
  \end{itemize}
\begin{verbatim}
MatMPIAIJSetPreallocation
  (Mat B,
   PetscInt d_nz,const PetscInt d_nnz[],
   PetscInt o_nz,const PetscInt o_nnz[])
\end{verbatim}

In Fortran use \n{PETSC_NULL_INTEGER} if not specifying arrays
}

\begin{details}
\frame[containsverbatim]{\frametitle{Matrix creation all in one}
\begin{verbatim}
MatCreateSeqAIJ(MPI_Comm comm,PetscInt m,PetscInt n,
  PetscInt nz,const PetscInt nnz[],Mat *A)
MatCreateMPIAIJ(MPI_Comm comm,
  PetscInt m,PetscInt n,PetscInt M,PetscInt N,
  PetscInt d_nz,const PetscInt d_nnz[],
  PetscInt o_nz,const PetscInt o_nnz[],
  Mat *A)
\end{verbatim}
}
\end{details}

\frame[containsverbatim]{\frametitle{Querying parallel structure}

Matrix partitioned by block rows:
\begin{verbatim}
MatGetSize(Mat mat,PetscInt *M,PetscInt* N);
MatGetLocalSize(Mat mat,PetscInt *m,PetscInt* n);
MatGetOwnershipRange(Mat A,int *first row,int *last row);
\end{verbatim}
 In query functions, unneeded components can be specified as
 \n{PETSC_NULL}.\\
 Fortran: \n{PETSC_NULL_INTEGER}
}

\frame[containsverbatim]{\frametitle{Setting values}

Set one value:
\begin{verbatim}
MatSetValue(Mat A,
  PetscInt i,PetscInt j,PetscScalar va,InsertMode mode)
\end{verbatim}
where insert mode is \n{INSERT_VALUES, ADD_VALUES}

Set block of values:
\begin{verbatim}
MatSetValues(Mat A,int m,const int idxm[],
    int n,const int idxn[],const PetscScalar values[],
    InsertMode mode)
\end{verbatim}
(\n{v} is row-oriented)
}

\frame[containsverbatim]{
Special case of the general case:
\begin{verbatim}
MatSetValues(A,1,&i,1,&j,&v,INSERT_VALUES); // C
MatSetValues(A,1,i,1,j,v,INSERT_VALUES,e); ! F
\end{verbatim}
}

\frame[containsverbatim]{\frametitle{Assembling the matrix}
Setting elements is independent of parallelism; move elements to
proper processor:
\begin{verbatim}
MatAssemblyBegin(Mat A,MAT_FINAL_ASSEMBLY);
MatAssemblyEnd(Mat A,MAT_FINAL_ASSEMBLY);
\end{verbatim}

Cannot mix inserting/adding values: need to do assembly in between
with \n{MAT_FLUSH_ASSEMBLY}
}

\begin{exerciseframe}[matvec]
  Pretend that you do not know how the matrix is created. Use
  \n{MatGetOwnershipRange} or \n{MatGetLocalSize} to create a vector
  with the same distribution, and then compute $y\leftarrow Ax$.

  (Part of the code has been disabled with \verb+#if 0+. We will get
  to that next.)
\end{exerciseframe}

\begin{details}
\frame[containsverbatim]{\frametitle{Getting values (C)}
  \begin{itemize}
  \item Values are often not needed: many matrix operations supported
  \item Matrix elements can only be obtained locally.
  \end{itemize}
\begin{verbatim}
PetscErrorCode  MatGetRow(Mat mat,
 PetscInt row,PetscInt *ncols,const PetscInt *cols[],
 const PetscScalar *vals[])
PetscErrorCode MatRestoreRow(/* same parameters */
\end{verbatim}
Note: for inspection only; possibly expensive.
}

\frame[containsverbatim]{\frametitle{Getting values (F)}
\begin{verbatim}
  MatGetRow(A,row,ncols,cols,vals,ierr)
  MatRestoreRow(A,row,ncols,cols,vals,ierr)
\end{verbatim}
where \n{cols(maxcols), vals(maxcols)} are long enough arrays
(allocated by the user)
}

\begin{exerciseframe}[matvec]
  Advanced exercise: create a sequential (uni-processor)
  vector. Question: how does the code achieve this?
  Give it the data of the distributed vector. Use that to compute the
  vector norm on each process separately.

  (Start by removing the \verb+#if 0+ and \verb+#endif+.)
\end{exerciseframe}

\frame[containsverbatim]{\frametitle{Other matrix types}

\n{MATBAIJ} : blocked matrices (dof per node)

(see \n{PETSC_DIR/include/petscmat.h})

Dense:
\begin{verbatim}
MatCreateSeqDense(PETSC_COMM_SELF,int m,int n,
  PetscScalar *data,Mat *A);
MatCreateDense(MPI_Comm comm,
  PetscInt m,PetscInt n,PetscInt M,PetscInt N,
  PetscScalar *data,Mat *A)
fg\end{verbatim}
Data argument optional:
\n{PETSC_NULL} or \n{PETSC_NULL_SCALAR} causes allocation
}
\end{details}

\frame[containsverbatim]{\frametitle{Matrix operations}
Main operations are matrix-vector:
\begin{verbatim}
MatMult(Mat A,Vec in,Vec out);
MatMultAdd
MatMultTranspose
MatMultTransposeAdd
\end{verbatim}

Simple operations on matrices:
\begin{verbatim}
MatNorm

MatScale
MatDiagonalScale
\end{verbatim}
}

\frame[containsverbatim]{\frametitle{Some matrix-matrix operations}
\begin{verbatim}
 MatMatMult(Mat,Mat,MatReuse,PetscReal,Mat*);

 MatPtAP(Mat,Mat,MatReuse,PetscReal,Mat*);

 MatMatMultTranspose(Mat,Mat,MatReuse,PetscReal,Mat*);

 MatAXPY(Mat,PetscScalar,Mat,MatStructure);
\end{verbatim}
}

\begin{details}
\frame[containsverbatim]{\frametitle{Matrix viewers}
\begin{verbatim}
MatView(A,PETSC_VIEWER_STDOUT_WORLD);

row 0: (0, 1)  (2, 0.333333)  (3, 0.25)  (4, 0.2)
row 1: (0, 0.5)  (1, 0.333333)  (2, 0.25)  (3, 0.2)
....
\end{verbatim}
(Fortran: \n{PETSC_NULL_INTEGER})
  \begin{itemize}
  \item also invoked by \n{-mat_view}
  \item Sparse: only allocated positions listed
  \item other viewers: for instance \n{-mat_view_draw} (X~terminal)
  \end{itemize}

}
\end{details}

\begin{longversion}
\frame[containsverbatim]{\frametitle{General viewers}
  Any PETSc object can be `viewed'
  \begin{itemize}
  \item Terminal output: useful for vectors and matrices but also for
    solver objects.
  \item Binary output: great for vectors and matrices.
  \item Viewing can go both ways: load a matrix from file or URL into
    an object.
  \item Viewing through a socket, to Matlab or Mathematica, HDF5, VTK.
  \end{itemize}
\begin{verbatim}
PetscViewer fd;
PetscViewerCreate( comm, &fd );
PetscViewerSetType( fd,PETSCVIEWERVTK );
MatView( A,fd );
PetscViewerDestroy(fd);
\end{verbatim}
}

%% \begin{exerciseframe}[viewer]
%%   Fill in the blanks.
%% \end{exerciseframe}

%\subsection{Shell matrices}

\frame[containsverbatim]{\frametitle{Shell matrices}
What if the matrix is a user-supplied operator, and not stored?

\begin{verbatim}
MatSetType(A,MATSHELL); /* or */
MatCreateShell(MPI Comm comm,
    int m,int n,int M,int N,void *ctx,Mat *mat);

PetscErrorCode UserMult(Mat mat,Vec x,Vec y);

MatShellSetOperation(Mat mat,MatOperation MATOP_MULT,
  (void(*)(void)) PetscErrorCode (*UserMult)(Mat,Vec,Vec));
\end{verbatim}
Inside iterative solvers, PETSc calls \n{MatMult(A,x,y)}: \\
no difference between stored matrices and shell matrices
}

\frame[containsverbatim]{\frametitle{Shell matrix context}
Shell matrices need custom data
\begin{verbatim}
MatShellSetContext(Mat mat,void *ctx);
MatShellGetContext(Mat mat,void **ctx);
\end{verbatim}
(This does not work in Fortran: use Common or Module
or write interface block)

User program sets context, matmult routine accesses it
}

\frame[containsverbatim]{\frametitle{Shell matrix example}
\begin{verbatim}
...
MatSetType(A,MATSHELL);
MatShellSetOperation(A,MATOP_MULT,(void*)&mymatmult);
MatShellSetContext(A,(void*)&mystruct);
...

PetscErrorCode mymatmult(Mat mat,Vec in,Vec out)
{
  PetscFunctionBegin;
  MatShellGetContext(mat,(void**)&mystruct);
  /* compute out from in, using mystruct */
  PetscFunctionReturn(0);
}
\end{verbatim}
}

%\subsection{More matrix topics}

\frame[containsverbatim]{\frametitle{Submatrices}
Extract one parallel submatrix:
\begin{verbatim}
MatGetSubMatrix(Mat mat,
  IS isrow,IS iscol,PetscInt csize,MatReuse cll,
  Mat *newmat)
\end{verbatim}
Extract multiple single-processor matrices:
{\small
\begin{verbatim}
MatGetSubMatrices(Mat mat,
  PetscInt n,const IS irow[],const IS icol[],MatReuse scall,
  Mat *submat[])
\end{verbatim}
}
Collective call, but different index sets per processor
}

\frame[containsverbatim]{\frametitle{Load balancing}

\begin{verbatim}
MatPartitioningCreate
    (MPI Comm comm,MatPartitioning *part);
\end{verbatim}
Various packages for creating better partitioning: Chaco, Parmetis

% MatCreateMPIAdj(MPI Comm comm,int mlocal,int n,const int ia[],const int ja[],
% int *weights,Mat *Adj);
% MatPartitioningCreate(MPI Comm comm,MatPartitioning *part);
% MatPartitioningSetAdjacency(MatPartitioning part,Mat Adj);
% MatPartitioningSetFromOptions(MatPartitioning part);
% MatPartitioningApply(MatPartitioning part,IS *is);
% MatPartitioningDestroy(MatPartitioning part);
% MatDestroy(Mat Adj);
% ISPartitioningToNumbering(IS is,IS *isg);


}

\end{longversion}
\endinput

\subsection{Example}

\frame[containsverbatim]{\frametitle{Example: stencil matrices}
\includegraphics[scale=.6]{laplacedomain}
domain, gives matrix:
\includegraphics[scale=.6]{laplacematrix}
}

\frame[containsverbatim]{\frametitle{Example: stencil matrices}
\small
\begin{verbatim}
for ( i=0; i<m; i++ ) {
  for ( j=0; j<n; j++ ) {
    v = -1.0;  I = j + n*i;
    if ( i>0 ) {
        J = I - n;
        ierr = MatSetValues(mat,1,&I,1,&J,&v,INSERT_VALUES);
    }
    if ( i<m-1 ) {
        J = I + n;
        ierr = MatSetValues(mat,1,&I,1,&J,&v,INSERT_VALUES);
    }
    if ( j>0 )   { /* blah */
    if ( j<n-1 ) { /* blah */
    v = 4.0;
    ierr = MatSetValues(mat,1,&I,1,&I,&v,INSERT_VALUES);
  }
}
\end{verbatim}
}
\end{longversion}

\frame[containsverbatim]{\frametitle{Matrices from user data}
\footnotesize
\begin{verbatim}
MatCreateSeqAIJWithArrays(MPI_Comm comm,
  PetscInt m,PetscInt n,
  PetscInt* i,PetscInt*j,PetscScalar *a,Mat *mat)
MatCreateMPIAIJWithArrays(MPI_Comm comm,
  PetscInt m,PetscInt n,PetscInt M,PetscInt N,
  const PetscInt i[],const PetscInt j[],const PetscScalar a[],
  Mat *mat)
MatCreateMPIAIJWithSplitArrays(MPI_Comm comm,
  PetscInt m,PetscInt n,PetscInt M,PetscInt N,
  PetscInt i[],PetscInt j[],PetscScalar a[],
  PetscInt oi[], PetscInt oj[],PetscScalar oa[],
  Mat *mat)
\end{verbatim}
Data is copied in the 2nd case.
}

\begin{shortversion}
\frame[containsverbatim]{\frametitle{More matrix topics}
  \begin{itemize}
  \item Viewers are also for binary dump, plotting with~X
  \item Shell matrices: matrix-free operation
  \item Submatrix extraction
  \item Matrix partitioning for load balancing
  \end{itemize}
}
\end{shortversion}
